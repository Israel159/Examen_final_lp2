Asesor,Nombre de tesista,Año de publicación,Resumen,Institución,Titulo de la tesis,Grado
"Salinas Flores, Jesús Walter||Rosas Villena, Fernando René","Rado Huaringa, Joao Manuel",2014,"El objetivo de la investigación fue probar la hipótesis que la tasa de error de clasificación utilizando  el análisis discriminante con algoritmos genéticos es menor a la que se obtiene con el análisis discriminante lineal de Fisher. La aplicación se efectuó en la predicción del rendimiento en el examen de admisión de la Universidad Nacional Agraria La Molina de los postulantes cuya preparación se realizó en su Centro de Estudios Preuniversitarios. En la técnica de algoritmos genéticos  se empleó el método de selección, cruce y mutación que permitió realizar la búsqueda de funciones discriminantes con error mínimo. Los resultados del estudio indican que el análisis discriminante con algoritmos genéticos proporcionó una función discriminante más eficiente que la proporcionada por Fisher.",Universidad Nacional Agraria La Molina,Predicción del rendimiento en el exámen de admisión a la UNALM [Universidad Nacional Agraria La Molina] utilizando las técnicas de análisis discriminante lineal y análisis discriminante con algoritmos genéticos,Ingeniero Estadístico e Informático
"Porras Cerrón, Jaime Carlos","Rado Huaringa, Joao Manuel",2019,"En esta investigación se realizó la aplicación de los métodos de equiparación lineal y equipercentil a los puntajes obtenidos de los postulantes a los exámenes de admisión 2016-I y 2016-II de la Universidad Nacional Agraria La Molina. El desarrollo se realizó en las seis áreas que se evalúan en el examen de admisión: Razonamiento Verbal, Razonamiento Matemático, Matemática, Física, Química y Biología. El indicador utilizado para comparar ambos métodos fue el error estándar de equiparación. Entre los resultados más importantes se encontró que el método de equiparación lineal tuvo un mejor ajuste que el método equipercentil. Respecto a la dificultad de los exámenes de admisión, se obtuvo que el examen 2016-II presentó una mayor dificultad que el examen 2016-I. Finalmente, en relación a las seis áreas evaluadas en los exámenes, fue Matemática la que presentó una mayor dificultad en el examen de admisión 2016-II que en el 2016-I.",Universidad Nacional Agraria La Molina,Equiparaciòn de puntuaciones en el examen de admisión de la Universidad Nacional Agraria La Molina utilizando los métodos lineal y equipercentil,Magister Scientiae - Estadística Aplicada
"Rosas Villena, Fernando René","Vivanco Huaytara, Fredy",2018,"El objetivo general de la investigación es la identificación entre las técnicas del Análisis Discriminante Lineal de Fisher y Máquina de Soporte Vectorial la que presenta mejores indicadores de clasificación del rendimiento de los postulantes en la prueba de admisión 2015-II en la Universidad Nacional Agraria La Molina (UNALM). Ambas técnicas estadísticas se aplicaron en dos oportunidades, en la primera se evaluó el resultado en la prueba admisión de la UNALM de los postulantes que no se prepararon en el CEP-UNALM y que ingresaron o no a la universidad (aplicación 1) y en la segunda se evaluó resultado en la prueba admisión de la UNALM de los postulantes que si se prepararon en el CEP-UNALM y que ingresaron o no a la universidad (aplicación 2). Los resultados muestran que la técnica Máquina de Soporte Vectorial presenta mejores indicadores de clasificación que el Análisis Discriminante Lineal de Fisher.",Universidad Nacional Agraria La Molina,Clasificación de resultados en la prueba de admisión de la UNALM utilizando análisis discriminante lineal de Fisher y Máquina de Soporte Vectorial,Ingeniero Estadístico e Informático
"Rosas Villena, Fernando René","Salazar Vega, Rolando Jesús",2019,"El propósito principal de la investigación fue comprobar si el rendimiento académico de los estudiantes en el curso de Estadística General de la Universidad Nacional Agraria La Molina (UNALM) es explicado a través de un modelo propuesto de Ecuación Estructural de tres factores. El primero denominado “desempeño docente”, medido por las variables: planificación del curso, dominio del curso, métodos y recursos de instrucción, obligaciones docentes, método evaluativo, y motivación e interacción con los alumnos; el segundo llamado “autoconcepto”, medido por las variables: académico/laboral, social, emocional, familiar y físico y finalmente el tercero “rendimiento pasado”, medido a través del promedio ponderado acumulado. Los datos utilizados corresponden a las notas de los alumnos matriculados en el ciclo académico 2014-I en el curso de Estadística General, al promedio ponderado acumulado; y los valores se registraron en la escala de Likert de 1 al 10 de las encuestas de desempeño docente y autoconcepto. Estos dos instrumentos, cumplen con los requisitos de confiabilidad y validez al registrar en ambos casos indicadores por encima de los mínimos aceptables. El modelo de ecuación estructural propuesto fue reespecificado (mejorado) mediante la inclusión de una nueva relación de interdependencia, el rendimiento pasado como predictor del autoconcepto. Se verificó el ajuste del modelo de ecuación estructural reespecificado a través de los principales indicadores de ajuste absoluto e incremental. Entre los resultados más importantes de la investigación se verificó que el factor rendimiento pasado es el mejor predictor del factor rendimiento académico de los estudiantes en el curso de Estadística General y que los factores desempeño docente y rendimiento pasado explican al factor autoconcepto.",Universidad Nacional Agraria La Molina,Modelo de ecuación estructural explicativo del rendimiento académico de los estudiantes del curso de estadística general en la UNALM,Magister Scientiae - Estadística Aplicada
"Porras Cerrón, Jaime Carlos","Sucari Sucari, Ruben Elvis",2018,"En la presente tesis se desarrolló el método de clasificación llamado Análisis Discriminante No Métrico, y se comparó su desempeño con el Árbol de Clasificación CHAID y la Regresión Logística Multinomial, los cuales también son métodos que no necesitan la condición de normalidad multivariada, linealidad ni varianza homogénea para las variables independientes. Esta comparación de desempeño fue evaluado mediante la Validación Cruzada. Para la realización del estudio comparativo de estos clasificadores se utilizó conjuntos de datos que son proporcionados por la Universidad de California Irving (UCI). Se concluye que la Regresión Logística Multinomial tiene mejor desempeño en la clasificación de datos teniendo en cuenta la tasa de clasificación promedio y el tiempo de procesamiento",Universidad Nacional Agraria La Molina,"Comparación del análisis discriminante no métrico, árboles de clasificación Chaid y la regresión logística multinormal",Magister Scientiae - Estadística Aplicada
,"Porras Huamán, Beatriz Eta",2014,"El turismo en el Perú ha venido creciendo rápidamente en los últimos años MINCETUR - Plan Estratégico Nacional de Turismo 2012 - 2021 – PENTUR, pág. 11). En el proceso de la investigación turística se recopila los datos, para ser analizados y efectuar una crítica, con fines de alcanzar elementos de trabajo hacia otras fases de desarrollo turístico, las mismas que permitirán obtener nuevos conocimientos que como aporte se utilizarán en la actividad turismo.En este sentido el presente trabajo busca caracterizar o establecer cuáles son los perfiles de los turistas internos en el Perú de los niveles socioeconómicos medio y alto con la finalidad de diversificar la estrategia turística y de este modo poder administrar en forma sostenida recursos del sector público como el privado. Esta investigación utiliza una muestra de 2,400 turistas de los principales destinos turísticos del Perú. El objetivo planteado para el desarrollo del presente trabajo es la determinación de los grupos de individuos observando sus características socio-demográficas, los hábitos de vida y las preferencias con respecto a viajes al interior del país. La metodología empleada corresponde a la técnica de agrupamiento Análisis de Conglomerados Bietápico. La pregunta de investigación planteada para el desarrollo del proyecto se orienta a determinar los diversos tipos de perfiles de turistas nacionales en los niveles socioeconómicos A, B y C. Finalmente se determinó la homogeneidad de los grupos en función a la variabilidad intra-grupos, encontrándose dos grupos, descritos estos en función a los estadísticos encontrados dentro de cada grupo.",Universidad Nacional Agraria La Molina,Determinación de perfiles de turistas nacionales de los niveles socioeconómicos medio y alto mediante el análisis conglomerado bietápico,Ingeniero Estadístico e Informático
,"Ramírez Soplin, Magally Loidit",2014,"El presente estudio de investigación se centró en identificar los perfiles más adecuados, en una muestra de 8, 504 clientes que realizaron transacciones crediticias en el primer trimestre del año. Se agruparon los casos mediante las técnicas de segmentación: K-means, Bietápico y Kohonen, utilizando variables cuantitativas y categóricas. De las tres técnicas, la que obtuvo mayor medida de silueta de cohesión y separación, fue K-means, indicando una estructura “buena” en cuanto a la cohesión al interior de los grupos y la separación de los mismos. Por otro lado, también se analizó las proporciones de los conglomerados, siendo la técnica K-means la que presentó las proporciones más adecuadas en función a las variables de historial crediticio y transacciones realizadas. Posterior a la obtención de los conglomerados, se procedió al proceso de obtención de la reglas de clasificación, mediante la técnica de regresión logística multinomial, la cual nos permitirá realizar predicciones futuras. El procedimiento se aplicó a la muestra particionada, es decir, una parte de entrenamiento y otra de comprobación. Finalmente, se obtuvo una adecuada tasa de eficiencia en ambas muestras. Además, los análisis permitieron identificar a dos conglomerados que muestran una alerta para la empresa, es decir necesitan ser gestionados de forma oportuna, ya que constituyen un futuro comportamiento de no pago de acuerdo a la caracterización obtenida de dichos conglomerados.",Universidad Nacional Agraria La Molina,Identificación de perfiles de clientes crediticios aplicando técnicas de segmentación y regresión logística multinomial,Ingeniero Estadístico e Informático
"Espinoza Villanueva, Luis Enrique","Linares Torres, Miguel Angel",2018,"Se realizó una investigación cuantitativa, con el fin de identificar los factores más influyentes al momento de evaluar el servicio que ofrece el centro educativo, el instrumento de medición es la encuesta vía telefónica y se  entrevistó a los  padres de familia que matricularon a sus hijos en el año 2016. La recolección de información se realizó por medio de un cuestionario,  seguidamente se analizó en una base de datos y con procedimientos estadísticos se ejecutó pruebas para determinar  qué factores  son los más influyentes al momento de evaluar la calidad del servicio. Asimismo, se analizó el entorno como la infraestructura, calidad de servicio, equipamiento tecnológico y calidad educativa.  Finalmente con los resultados de las pruebas se procedió a elaborar estrategias de mercado para un óptimo posicionamiento",Universidad Nacional Agraria La Molina,Estrategias de mercado en un centro educativo privado en la localidad de Arequipa,Ingeniero Estadístico e Informático
,"Vicente Vasquez, Juana Mercedes",2014,"El estudio tiene como objetivo clasificar a las familias encuestadas en los distritos de San Pablo, San Luis y San Bernardino de la provincia de San Pablo en el departamento de Cajamarca según un conjunto de variables socio-económicas. Estos datos corresponden a una investigación realizada por un grupo de personas que laboran en la Universidad del Pacifico, la encuesta fue realizada en Diciembre del 2006. Se desea clasificar a las familias para poder brindar un mejor control en el estudio longitudinal de los proyectos a ser evaluados. Para esto, al culminar la encuesta se planteó una clasificación preliminarmente la existencia de 4 grupos de familias. Para verificar esta clasificación se utilizó el “Análisis de Clúster”, que es un método multivariado de clasificación. Para el procesamiento de los datos se utilizó el programa “Minitab versión 17” y “Microsoft Excel”",Universidad Nacional Agraria La Molina,Clasificación de familias en Cajamarca según su situación económica mediante el análisis de conglomerados,Ingeniero Estadístico e Informático
,"Huamaní Miranda, María Alejandra",2014,"La realidad competitiva que en estos días enfrentan las entidades bancarias ha provocado que éstas no sólo concentren sus esfuerzos de marketing exclusivamente en estrategias de captación de clientes, sino también en estrategias de retención y ﬁdelización; la fuga de clientes es una situación que afecta la rentabilidad de la gran mayoría de las instituciones bancarias dado que se invierte mucho más en la captación de clientes que en campañas para la retención, por ello, es un tema de intensivo estudio cientíﬁco en los últimos años. Las entidades bancarias requieren contar con herramientas que les permitan estimar probabilidades de fuga para su cartera de clientes y así decidir sobre que clientes concentrar sus esfuerzos de retención. En el presente trabajo se utilizó la regresión logística de respuesta binaria  y el algoritmo de árbol de clasificación CART para predecir y clasificar a los clientes con riesgo de fuga y así identificar el mejor modelo explicativo de retención de clientes con riesgo de fuga para una entidad bancaria. El modelo que mejor explica el riesgo de fuga de un cliente fue la Regresión Logística binaria que obtuvo como variables predictoras número de transacciones, ingreso bruto, número de tarjetas usadas y línea de crédito. Las variables identificadas permitirán a la entidad bancaria reorientar las estrategias en las campañas de retención de clientes.",Universidad Nacional Agraria La Molina,Identificación de un modelo explicativo de retención de clientes con riesgo de fuga para una entidad bancaria aplicando regresión logística y árboles de clasificación CART,Ingeniero Estadístico e Informático
"Chue Gallardo, Jorge","Fernández Vásquez, Richard Fernando",2018,"En la actualidad las entidades bancarias conviven con clientes que no cumplen con sus obligaciones crediticias y se exceden del plazo estipulado acordado con el banco, a estos clientes se les denomina clientes morosos, por tal motivo el objetivo del presente trabajo es determinar el modelo de regresión binaria bayesiano con enlace asimétrico más adecuado para clasificar a los clientes que incumplirán sus pagos de sus tarjetas de crédito según sus probabilidades de mora en la entidad bancaria UNIBANK y haciendo uso de las variables más significativas. Se realizó un análisis comparativo entre los modelos de regresión bayesiana con enlaces asimétricos cloglog, power logit y scobit, y se determinó que el modelo de regresión binaria bayesiano con enlace asimétrico cloglog fue el más adecuado para clasificar a los clientes que incumplen sus obligaciones crediticias con sus tarjetas de crédito en la entidad bancaria UNIBANK según su probabilidad de mora, pues este modelo presentó un valor mucho mayor de sensibilidad que los modelos power logit y scobit, siendo las diferencias 8.5% y 9.1%, respectivamente",Universidad Nacional Agraria La Molina,Regresión bayesiana con enlaces asimétricos para la clasificación de clientes con propensión a caer en mora en una entidad bancaria,Magister Scientiae - Estadística Aplicada
"Miranda Villagomez, Clodomiro Fernando","Rubio Donet, Jorge Luis",2018,"La presencia de observaciones atípicas en un conjunto de datos es una de las causas que generan distorsiones en el análisis. La detección de dichas observaciones puede ayudar a una correcta evaluación de las tendencias en el comportamiento de los datos. Para el caso de datos multivariados se han desarrollado diversos métodos que permiten la detección de comportamientos atípicos, basados en métodos gráficos, y otros asumiendo una distribución normal multivariada. No obstante, en muchos casos el supuesto de normalidad multivariada no se cumple. El presente trabajo propone una prueba no paramétrica basada en la aplicación del método Bootstrap, utilizando como indicador de similitud a las distancias entre las representaciones obtenidas con series finitas de Fourier, propuesta por Andrews. El método propuesto permite detectar datos multivariados atípicos, combinando la significación estadística de la prueba Bootstrap y el análisis gráfico sugerido por Andrews, y que puede ser también aplicado a datos medidos en una escala ordinal. El método fue aplicado a cuatro conjuntos de datos, encontrando resultados satisfactorios en todos los casos.",Universidad Nacional Agraria La Molina,Detección de datos multivariados atípicos con series finitas de Fourier,Magister Scientiae - Estadística Aplicada
,"Neira Campos, Mike Alex",2017,"Este trabajo monográfico gira en torno a las series de tiempo con Redes Neuronales Artificiales a fin de realizar pronósticos. Para este propósito, el presente trabajo se compone de 4 capítulos, donde el primer capítulo versa sobre las definiciones y conceptos principales del pronóstico de una serie temporal que otorga validez teórica a la investigación. En el segundo capítulo, el lector podrá encontrar la descripción de las Redes Neuronales Artificiales en la predicción de datos. El tercer capítulo, da cuenta de las implicaciones de una metodología del pronóstico de datos, utilizando las Redes Neuronales Artificiales. Finalmente, el capitulo 4, dilucida la aplicabilidad de las mencionadas Redes y se hace un paralelo con otros métodos de pronóstico con el objeto de resaltar sus diferencias y características. En conclusión, podemos decir que el lector podrá encontrar en este trabajo las etapas necesarias para llevar a cabo la elaboración de una red neuronal que pueda predecir valores futuros de una serie de tiempo.",Universidad Nacional Agraria La Molina,Descripción metodológica de las series de tiempo con redes neuronales artificiales,Ingeniero Estadístico e Informático
,"Flores Espinoza, María del Carmen",2014,"El presente trabajo tiene como propósito esencial, realizar una segmentación psicográfica, en base a valores y estilos de vida, así como las actitudes frente a la tecnología de los habitantes de Lima Metropolitana y Callao de los niveles socioeconómicos A, B, C y D, entre 15 y 69 años de edad. Se utilizó una muestra de 906 casos, a los cuales se le aplicó un cuestionario estructurado que contenía 35 atributos previamente definidos en base a una investigación cualitativa, y otras preguntas de control, tal como edad, nivel socioeconómico y tenencia de productos tecnológicos en el hogar. Del total de 35 atributos evaluados los cuales se redujeron en 8 factores tras realizar un análisis factorial. Seguidamente se realizó un análisis clúster, donde se identificaron 5 segmentos caracterizados por los atributos previamente mencionados: Aquellos que se encuentran Orientados a la tecnología; los que, a pesar de tener productos tecnológicos prefieren estar esconectados; los que se encuentran Orientados al poder, los que tienden a ser Tradicionales por sus actitudes frente a las costumbres, y aquellos Orientados a los valores que tienen como eje virtudes tales como la solidaridad, sencillez, etc.",Universidad Nacional Agraria La Molina,Segmentación de usuarios de productos tecnológicos a partir de valores y actitudes,Ingeniero Estadístico e Informático
,"Acosta Pizarro, Diana Rosa",2014,"El presente estudio tiene como objetivo principal identificar el perfil de los clientes del departamento de Lima que aceptan una tarjeta de crédito de una entidad financiera cuando el productoes ofrecido por el canal de ventas Call Center. Se utilizó la técnica de Árboles de Clasificación CHAID Exhaustivo el cual proporciona buenos resultados de clasificación correcta de los clientes que aceptan una tarjeta de crédito vía Call Center. Se consideró una muestra en un período de cinco meses (Diciembre 2013 a Abril 2014) logrando identificar que las variables más significativas que aportan en el modelo son la edad, el ingreso neto mensual y el tipo de tarjeta que se le ofrece al cliente. Estas variables presentan importancia relevante en el cliente para tomar la decisión de aceptar una tarjeta de crédito. Los resultados obtenidos mediante el algoritmo CHAID Exhaustivopermitieron identificar los patrones que definen el perfil de los clientes que aceptan una tarjeta de crédito vía Call center con el fin de ser más efectivos, aumentando el número de ventas, reduciendo el número de llamadas, minimizando costos y tiempo",Universidad Nacional Agraria La Molina,Perfil de los clientes que aceptan una tarjeta de crédito de un banco via Call Center utilizando el algoritmo Chaid exhaustivo,Ingeniero Estadístico e Informático
"López de Castilla Vásquez, Carlos","Mendoza  Quevedo, Diego Alonso",2013,"La presente investigación tiene por objetivo principal determinar si las distribuciones asimétricas skew-normal y skew-tson buenos modelos para describir datos relativos al coste de los siniestros. Para lo cual se analizaron dos conjuntos de datos de una compañía de seguros, ajustando las distribuciones en estudio y las tradicionales a estos datos. Luego se compararon las distribuciones empleando el Criterio de Información de Akaike (AIC) y del Logaritmo de la función de Verosimilitud, complementados con la prueba de bondad de ajuste Kolmogorov-Smirnov,obteniendo resultados positivos. Además, se calcularon medidas de riesgo como el Valor en Riesgo (VaR) y el Valor en Riesgo Condicional (TVaR), que brindaron mayor solidez a los resultados previos: los costes de los siniestros en los seguros se ajustan bien a las distribuciones asimétricas skew-normal y skew-t.",Universidad Nacional Agraria La Molina,Análisis del coste de los siniestros en una compañía de seguros utilizando las distribuciones asimétricas Skew-Normal y Skew-T,Ingeniero Estadístico e Informático
"Soto Rodríguez, Iván Dennys","Cormán Trujillo, Juan",2020,"La entidad pública brinda servicios de capacitación para el Sector Vivienda, Construcción y Saneamiento, inicia su vida institucional en el año 1977. Tiene como finalidad la formación de los trabajadores del sector construcción, la educación superior no universitaria, el desarrollo de investigaciones vinculadas a la problemática de la vivienda y edificación, así como la propuesta de normas técnicas de aplicación nacional. Y se ha establecido en doce sedes a nivel nacional y capacita a más de 30,000 trabajadores en el desarrollo de nuevas tecnologías. La entidad según su estructura organizacional se encuentra conformado por: Consejo Directivo, Gerencia General, Oficina de Administración y Finanzas, Oficina de Planificación y Presupuesto, Gerencia de Formación profesional y Gerencia de Investigación y Normalización; cabe señalar que el Departamento de Informática forma parte de la Oficina de Administración y Finanzas, en la cual se desempeñó el cargo.",Universidad Nacional Agraria La Molina,"Comportamiento del gasto público de una entidad pública usando el modelo Box - Jenkins, 2009 - 2017",Ingeniero Estadístico e Informático
,"Montaño Miranda, Beatriz del Carmen Lidia",2013,"El presente estudio tuvo como objetivo principal verificar la disponibilidad de productos en el almacén de un Supermercado, así las decisiones a tomar ante la falta de productos serían más certeras y se tendría un mejor panorama al respecto de la situación del abastecimiento del Supermercado. El trabajo consiste en un modelo de predicción de quiebres de stock para un supermercado. Se analizaron las ventas en unidades, el stock de los productos, los despachos de proveedores, los días del mes de Agosto (con ventas y sin ellas) de un grupo de productos de diferentes gerencias del supermercado (Abarrotes Comestibles, Abarrotes no Comestibles y Bebidas). Con la información recopilada se consideró el valor de la variable dependiente (en quiebre con valor 1 y O en caso contrario). La selección de variables por Boruta permitió obtener un modelo con menos cantidad de variables y con un mejor ajuste al realizar el análisis usando la regresión logística en comparación con la selección de variables por Stepwise.",Universidad Nacional Agraria La Molina,Modelo predictivo de quiebre de stock en un supermercado comparando dos métodos de selección de variables,Ingeniero Estadístico e Informático
"Rino Sotomayor, Ruíz","Hurtado Oliva, Katherine Vanessa",2015,"El presente trabajo tiene como objetivo estimar el modelo Vector Autorregresivo (VAR) que permita describir simultáneamente el comportamiento de la morosidad de cartera, el Producto Bruto Interno y la tasa de interés de las empresas financieras peruanas del crédito de consumo durante el período de octubre 2002 – marzo 2014. Así mismo, evaluar a través del análisis de impulso – respuesta y la descomposición de varianza, el impacto de cada una de las variables sobre otra y su contribución a la desviación típica del error. Al realizar el análisis exploratorio de las variables involucradas, se encontró que tanto el porcentaje de morosidad como la tasa de interés muestran una tendencia creciente desde el 2011 en adelante. Finalmente, el modelo Vector Autorregresivo (VAR) que mejor se ajusta, es aquel que considera las primeras diferencias finitas de cada una de las variables bajo cuatro números de rezagos.",Universidad Nacional Agraria La Molina,Análisis de la morosidad de cartera en empresas financieras peruanas para los créditos de consumo aplicando la metodología VAR,Ingeniero Estadístico e Informático
"Porras Cerrón, Jaime Carlos","Tang Bedoya, Felix Augusto||Vargas Cuyan, Cecilia",2016,"El aumento continuo de grandes volúmenes de datos y la importancia de la utilización de éstos, junto con la búsqueda de satisfacción de los clientes se han vuelto un gran reto que las grandes empresas hoy en día quieren superar. En la actualidad las empresas conocen la importancia que tiene el almacenamiento, la captura de datos y el beneficio que le puede resultar si se explotan correctamente. Este estudio presenta una técnica estadística para segmentar clientes de una tienda de retail en el Perú. El conjunto de datos está conformado por clientes (personas naturales) que han adquirido un artículo para su hogar mediante un crédito en el primer trimestre del año 2013 de una tienda de retail. El conjunto de datos inicial estaba compuesto por 6284 clientes. Luego de proceder con el análisis exploratorio y la limpieza de datos, correspondiente a la eliminación de datos outliers y faltantes; se trabajó un conjunto de datos compuesto por 4980 clientes. Se aplicó análisis de conglomerados bietápico, que es una técnica de segmentación que permite trabajar con variables cuantitativas y categóricas. El resultado de la aplicación de la técnica brindó 3 conglomerados: el primero con 1817 clientes (36.5%), el segundo conglomerado con 1390 clientes (27.9%) y  el tercer conglomerado con 1773 clientes (35.6%). Después de esto se procedió a describir los perfiles de cada conglomerado y se propusieron estrategias de marketing mix para cada uno de ellos, con el objetivo de fidelizar al cliente, aumentar las ventas y el posicionamiento de la tienda de retail.",Universidad Nacional Agraria La Molina,Segmentación de clientes de una tienda de electrodomésticos utilizando el análisis de conglomerados,Ingeniero Estadístico e Informático
"López de Castilla Vásquez, Carlos","Quispe Quispe, Braulio",2016,"La presente tesis plantea una aplicación de los modelos estadísticos de procesos puntuales espaciales Poisson así como de los modelos Clúster del tipo Neyman - Scott. Particularmente, se enfoca en evaluar la distribución espacial de hechos delictivos y su relación con algunas covariables espaciales. De esta forma se permitirá orientar y/o establecer políticas referidas a seguridad ciudadana de índole nacional y/o local. El área de estudio corresponde a los distritos de Lima Centro y Residencial, para lo cual se toma en cuenta la información de ubicaciones georreferenciadas de los hechos delictivos reportados por las víctimas a finales del año 2013 hasta inicios del 2014. Las ubicaciones de los delitos son representadas por puntos, y el conjunto de estos se consideran un patrón puntual, el cual representa una realización de un proceso puntual espacial subyacente en el espacio de estudio. El modelamiento estadístico se realiza a través de la intensidad de puntos, la cual puede ser estimada para cualquier ubicación específica del área de estudio y son los modelos log-lineales los más usados para representar su relación con un conjunto de covariables espaciales cuyos efectos podemos representar en un conjunto de parámetros; a estos modelos se les conocen como modelos paramétricos de procesos puntuales espaciales. Las estadísticas de resumen, conocidas también como propiedades de primer y segundo orden de un proceso puntual así como los métodos basados en distancia entre puntos, han sido aplicados con fines de realizar el análisis exploratorio y determinar: el tipo de distribución espacial (regular, aleatorio o clústeres) que siguen los hechos delictivos (patrón puntual), la distribución de la distancia de un punto arbitrario a un lugar de ocurrencia de un delito y de la distancia de un hecho delictivo a otro, entre otras. Finalmente, se concluye que la distribución espacial de los hechos delictivos en Lima, no es homogénea, existiendo clustering o agregación de puntos, los cuales se traducen en zonas con mayor incidencia de hechos delictivos y su intensidad guarda relación con la ubicación de los límites distritales, la inversión destinada al orden interno y la densidad poblacional.",Universidad Nacional Agraria La Molina,"Modelos estadísticos en procesos puntuales espaciales Poisson para evaluar la distribución espacial de los hechos delictivos en Lima, Perú",Magister Scientiae - Estadística Aplicada
"Maehara Oyata, Víctor Manuel","Rebaza Fernández, Diana del Rocío",2017,"La recurrencia de un evento en un paciente es la frecuencia observada de este en un periodo de tiempo durante el seguimiento al individuo, por ejemplo hospitalizaciones sucesivas de neumonía, episodios de epilepsia, recaídas de cáncer, entre otros. Los modelos de eventos recurrentes son muy útiles para la aplicación en estos fenómenos, y la presente investigación pretende ilustrar y comparar modelos particulares de datos de eventos recurrentes sin efecto aleatorio: Andersen y Gill (A-D); Wei, Lin y Weissfeld (WLW); y, Prentice, Williams  y Peterson (PWP), los cuales son modelos basados en la extensión de Cox de riesgos proporcionales, en estos modelos se asumen independencia de eventos. Otro modelo estudiado es el modelo de Fragilidad Compartida Gamma para eventos recurrentes que considera un término de fragilidad y asume que este término influye en la recurrencia de los eventos de un mismo sujeto. Para la estimación de los parámetros en los modelos sin efecto aleatorio se utilizó el método de máxima verosimilitud parcial mientras que para el modelo de fragilidad  fue el método de máxima verosimilitud penalizado, el cual penaliza la función de riesgo base. Los datos usados para la aplicación de estas metodologías fue proporcionada por el médico Ginecólogo Oncólogo Dr. Vladimir Villoslada Terrones del Instituto Nacional de Enfermedades Neoplásicas (INEN). Estos datos describen un conjunto de variables relacionados al cáncer de mama en una cohorte prospectiva de 68 pacientes con diagnóstico positivo, sometidos a una cirugía mastectomía. Al procesar y analizar los resultados obtenidos, se encontró que el modelo Andersen y Gill (A-D) y Prentice, Williams y Peterson (PWP) son los que ajustan mejor a este conjunto de datos. Entre los resultados encontrados se obtuvo que los factores asociados al riesgo de recurrencia de cáncer de mama son la edad de inicio al estudio, la edad de primera menstruación (menarquia) y tipo carcinoma lobulillar. Estos modelos presentan similares resultados debido a la significancia estadística en las variables y el cumplimiento del supuesto de riesgos proporcionales.",Universidad Nacional Agraria La Molina,Modelos semiparamétricos de eventos recurrentes: caso aplicación a pacientes con cáncer de mama,Magister Scientiae - Estadística Aplicada
"Menacho Chiok, César Higinio","Cano Alva Trinidad, Jesús María",2018,"Las instituciones de educación superior, cada vez más están implementando en sus diversos cursos evaluaciones virtuales vía web, con la finalidad de automatizar y medir con mayor precisión los conocimientos que van adquiriendo los estudiantes. La presente investigación, tiene como objetivo realizar un estudio comparativo de la Teoría Clásica del Test y la Teoría de Respuesta al Ítem, que pueden ser aplicados a los test informatizados con la finalidad de evaluar y medir las propiedades psicométricas y estadísticas, enfocando la confiabilidad y validez de los test de evaluación. Como caso de estudio, se usó un test informatizado de 30 preguntas (ítems) que se aplicó virtualmente a 775 estudiantes de una institución de educación superior matriculados en un curso de Estadística básica en el semestre 2016 II. El análisis de la confiabilidad, resultó con un alfa de Cronbach de 0.8325 pudiendo indicar una confiabilidad buena para la prueba informatizada, también fue corroborada con una correlación de Spearman-Brown de 0.815. El análisis con la TCT, el índice de dificultad identificó tres preguntas muy fáciles (V7, V8 y V12) que fueron retiradas, mientras el índice de discriminación no encontró ninguna pregunta con problemas. El supuesto de la unidimensionalidad de la prueba usando el análisis factorial fue probado con una variancia explicada del primer factor de 24.7%. El modelo logístico binario de la TRI que mejor se ajustó a los datos de la prueba fue el de tres parámetros (3PL). El proceso de calibración con el modelo 3PL, permitió retirar las preguntas V28 (índice de discriminación mayor 0.65) y V8, V12, V16 y V18 (índice del azar mayores a 0.4). Mientras que todas la preguntas estuvieron dentro del rango permitido para índice de dificultad",Universidad Nacional Agraria La Molina,Análisis comparativo de la teoría clásica de los test y la teoría de respuesta al ítem aplicadas en evaluaciones informatizadas,Magister Scientiae - Estadística Aplicada
"Menacho Chiok, César Higinio","Aquino Gamboa, Juan Carlos",2019,"En la presente tesis se aplica la regresión logística ordinal, con la finalidad de identificar las variables que mejor explican los rangos de los ingresos de los egresados universitarios del Perú. Para el estudio se usa los datos de la encuesta de egresados universitarios del año 2014 realizada por el INEI. Se ajustan los datos a tres modelos aplicando la regresión logística ordinal con función de enlace logit proporcional acumulativo y para las universidades públicas y privadas. La variable dependiente son los rangos de ingresos (Bajo, Medio, Alto y Muy alto) y los conjuntos de variables independientes agrupadas en cuatro categorías: Grupo A: socio-académicas (7), Grupo B: referidas a la evaluación de las competencias recibidas en la universidad (12), Grupo C: referidas a la importancia de las competencias para su desarrollo profesional (12) y Grupo D: respecto a los profesores de la carrera (5). Para explicar los rangos de los ingresos, la regresión logística ordinal identificó para los tres modelos y para las universidades públicas y privadas, variables significativas socio-académicas: sexo, pertenencia al cuadro de méritos, si obtuvo o no el título profesional, su primer empleo relacionado con la formación profesional. Respecto a la calificación sobre la preparación recibida en la universidad para el desarrollo de las competencias: para coordinar actividades, para los conocimientos básicos de otros campos (públicas) y para el dominio del área de disciplina, el utilizar herramientas informáticas básicas, el utilizar software específico de la carrera (privadas). Respecto a la importancia de las competencias para su experiencia laboral: redactar informes o documentos, tener conocimientos básicos de otros campos o disciplinas (públicas) y rendir bajo presión y cumplir con los objetivos y coordinar actividades (privadas).",Universidad Nacional Agraria La Molina,Variables que explican los rangos remunerativos del primer empleo de los egresados universitarios del Perú aplicando regresión logística ordinal,Ingeniero Estadístico e Informático
"Menacho Chiok, César Higinio","Reyna Colona, Tino Fabricio",2017,"En este tiempo presente, las empresas desarrollan diversos planes de trabajo conjunto de negocio, a modo de corporaciones, joint ventures, etc., y surge la necesidad de manejo eficiente y ordenado de volúmenes grandes de datos e intercambio de información entre las corporaciones con los fines de negocio visibles en el ámbito comercial. En tal sentido, el trabajo de investigación provee una metodología de desarrollo de un sistema de integración basado en Middleware Orientado a la Mensajería (MOM) que media y favorece la integración financiera entre dos entidades de ejemplo, una empresa farmacéutica y un banco. El propósito del trabajo de investigación es que, con la metodología presentada, se identifiquen los procesos clave y la dinamización de los mismos para representar el flujo de transacciones electrónicas para la debida integración entre las dos entidades señaladas anteriormente. La metodología consta de las secciones de: análisis del sistema, diseño del sistema, arquitectura y secciones referidas a la interconexión, mensajería e interfaces gráficas de usuario para la debida gestión del sistema. Los resultados de la aplicación de esta metodología son: el reconocimiento de los procesos clave según los objetivos del trabajo, la presentación de la dinamización de tales procesos, y comprender de manera preliminar lo referido a la interconexión entre las entidades presentadas, empresa farmacéutica y banco, definición de mensajes para las transacciones financieras entre las entidades señaladas y la visualización de la gestión del sistema por medio de interfaces gráficas de usuario. Tales resultados conducen a la idoneidad de la metodología en todas las secciones presentadas, abriendo el espacio para posteriores oportunidades de investigación en banca electrónica.",Universidad Nacional Agraria La Molina,Análisis y diseño de un sistema distribuido de pago middleware orientado a la mensajería entre una entidad bancaria y una empresa farmacéutica,Ingeniero Estadístico e Informático
"Salinas Flores, Jesús Walter","Pariona Huarhuachi, Jefferson Clauss",2017,"La retención de clientes ha tomado mucha importancia en los últimos años en las entidades financieras debido a la competencia agresiva por parte del sector, así como la autonomía del cliente en buscar mejores beneficios dentro de todas las ofertas que existen en el mercado bancario lo que se ve reflejado en el aumento de la tasa de clientes fugados. Ante esto se ha visto necesaria la implementación de técnicas estadísticas y/o técnicas de minería de datos, con la finalidad de construir un clasificador predictivo que pueda ayudar a identificar a clientes potenciales a fugarse. En muchos casos cuando se aplican técnicas de clasificación, es común que la clase a predecir ocurra con menor frecuencia que la otra clase: la presencia de datos desbalanceados. Es decir, se tiene menor número de clientes fugados que no fugados, lo cual representa un inconveniente debido a que el clasificador necesita datos suficientes de ambas clases para poder aprender de ellas y así alcanzar una buena predicción. En esta investigación se propone el algoritmo Syntetic Minority Over-sampling Technique (SMOTE) como solución a este problema. SMOTE crea instancias nuevas a partir de un sobre-muestreo de las instancias existentes, llevando la clase minoritaria a un número suficiente para ser considerada balanceada y la clase mayoritaria si es necesaria reducirla mediante sub-muestreo aleatorio. En la presente investigación se validarán tales beneficios con la construcción de un modelo de regresión logística binaria con datos desbalanceados con y sin la aplicación del algoritmo de SMOTE; con el fin predecir la fuga de clientes en una entidad financiera. Se usarán para medir la precisión, la curva ROC y elementos de la comprobación de tabla cruzada como la especificidad y la sensibilidad.",Universidad Nacional Agraria La Molina,Clasificación de fuga de clientes en una entidad financiera utilizando el algoritmo Smote para datos desbalanceados en una regresión logística,Ingeniero Estadístico e Informático
"Chue Gallardo, Jorge","Muñoz Muñoz, Emanuel Guillermo",2015,"El propósito de este trabajo es presentar y aplicar la técnica de redes neuronales para predecir el éxito de la compra de deuda de una entidad financiera a otra. La técnica de redes neuronales se sustenta en el perceptron multicapa y en el algoritmo de retropropagación. Se explica el uso de la técnica mostrando sus diferentes pasos: establecimiento de la estructura, la función de activación (sigmoidea), el paradigma de aprendizaje, el factor de aprendizaje, la regla de aprendizaje, el algoritmo de aprendizaje (retropropagación), el entrenamiento y la evaluación de la red neuronal. Se realizó un pre procesamiento para tener datos de calidad, con estos datos limpios se aplicó la técnica de redes neuronales en la clasificación y para evaluar esta clasificación se realizó el post procesamiento, se puede notar que todo este proceso es minería de datos. Lo más resaltante en las redes neuronales son los valores hallados para los parámetros, identificados a través del entrenamiento. Estos parámetros son denominados pesos dinámicos, en estos pesos está el conocimiento de la red neuronal para poder lograr la predicción. El error de clasificación que se obtiene al aplicar a los datos de prueba a la red neuronal entrenada con las especificaciones ya señaladas fue de 22.89% y 4.31% para la red de cuatro y dos neuronas en la capa de salida respectivamente. Al probar con más de siete neuronas se obtuvieron errores de clasificaciones similares o mayores, esto se logró con un mayor costo computacional. Al comparar los resultados de los mismos datos con la regresión logística, se obtuvieron estos errores de clasificación 22.72% y 4.31% para los datos con la variable respuesta de cuatro y dos clases respectivamente.También evaluaron los modelos de clasificación con la técnica de validación cruzada, en este caso se obtuvo  76.57% y 77.29% que son los porcentajes de la eficiencia de predicción de redes neuronales y regresión logística respectivamente, solo para los casos donde la variable respuesta de los datos era de cuatro niveles",Universidad Nacional Agraria La Molina,Aplicación de redes neuronales y regresión logística para predecir el éxito de la compra de deuda de una entidad financiera,Magister Scientiae - Estadística Aplicada
"Febres Huamán, Grimaldo","Orosco Gavilán, Juan Carlos",2019,"La presente investigación es de naturaleza aplicada, y tiene el objetivo de analizar y evaluar la metodología Bootstrap en modelos heterocedásticos aplicados en la predicción del Índice General de la Bolsa de Valores de Lima (IGBVL), periodo 2010 - 2014. Se presenta sucintamente, los conceptos básicos de series temporales, los procesos seriales heterocedásticos, la metodología Bootstrap y sus aplicaciones a la inferencia estadística y a las series temporales, donde se presenta el algoritmo para procesos heterocedásticos GARCH propuesto por Pascual et al. (2006) y generalizados para los modelos EGARCH y TGARCH. Con los procedimientos mostrados fueron obtenidas las predicciones mediante la metodología paramétrica y metodología Bootstrap, que fueron comparados con valores reales y finalmente fueron evaluados los desempeños de ambas metodologías. Del estudio se obtuvo que los modelos que mejor ajustan a la serie son los modelos ARMA(1,1)-GARCH(1,1), ARMA(1,1)-EGARCH(1,1) y ARMA(1,1)-TGARCH(1,1) cada uno de ellos con el supuesto de distribución t de Student con 5 grados de libertad de los residuales, el estudio comparativo mostró que la aplicación de la metodología Bootstrap en la serie de los retornos del Índice General de la Bolsa de Valores de Lima, permite obtener intervalos de predicciones con mayores e iguales amplitudes en algunos horizontes hacia adelante en comparación con la metodología paramétrica, y también permitió construir con un buen desempeño los intervalos de predicción para las volatilidades, así siendo esta una alternativa para la construcción de intervalos de predicción en los modelos GARCH, EGARCH y TGARCH.",Universidad Nacional Agraria La Molina,Uso de los modelos heterocedásticos con Bootstrap en el análisis del Índice General de la Bolsa de Valores de Lima,Magister Scientiae - Estadística Aplicada
"Gonzáles Chavesta, Celso","Bazán Baca, Juan Francisco",2018,"El estudio ha tenido como propósito determinar el método de muestreo apropiado, así como realizar estimaciones de la desnutrición crónica (DC) por edades, sexo y gestión educativa. Se ha empleado el muestreo por conglomerados (escuelas) combinado con estratificado (gestión educativa) y selección sistemática; en una muestra de 16 escuelas de gestión estatal y 20 de no estatal, obteniéndose información de 3000 alumnos del primero al cuarto grado de primaria, de la Región Callao. Se analizó la relación talla/edad con la referencia del National Center for Health Statistic (NCHS) de los Estados Unidos de América y de la Organización Mundial de Salud (OMS) considerando como niño en DC aquel con talla debajo de menos dos desviaciones estándar. Se determinó que la talla promedio de los alumnos, tuvo un incremento de 3 cm. al 2016, respecto al año 2005. La DC el año 2016 alcanza el 2.5% (1455 alumnos) con el patrón del NCHS y 2.9% (1724 alumnos) con el de la OMS. Según la referencia NCHS, la DC baja en 4.1% el año 2016 (2.5%) respecto al año 2005 (6.6%). En este período, también baja la DC por edades, sexo y gestión de la institución educativa. El Odds Ratios de Prevalencia (ORP) con el patrón NCHS por sexo y gestión disminuye el año 2016 respecto al año 2005 a 1.27 y 2.29 respectivamente; resulta que un estudiante este desnutrido crónico es 1.27 veces más probable si es hombre a que sea mujer, y que un estudiante este desnutrido crónico es 2.29 veces más probable si estudia en una institución educativa estatal a que estudie en una no estatal. Para tener alrededor del 96% de casos válidos, para las edades de 6 a 9 años en la encuesta de talla a escolares del primero al cuarto grado de primaria de menores, es recomendable efectuarla en el primer semestre del año, preferentemente los meses de mayo y junio",Universidad Nacional Agraria La Molina,Aplicación del muestreo para estimar la desnutrición crónica en escolares de 6 a 9 años en la Región Callao,Magister Scientiae - Estadística Aplicada
"Porras Cerrón, Jaime Carlos","Asencios Gonzalez, Zaida Beatriz",2019,"En la presente investigacion, se empleo Modelos de Ecuaciones Estructurales con Minimos Cuadrados Parciales (PLS-SEM por sus siglas en ingles), es una tecnica de segunda generacion que utiliza metodos estadisticos para el analisis simultaneo de relaciones complejas entre dos a mas constructos latentes. Se aplico PLS-SEM con el objetivo principal de comprender como o por que medios la violencia contra las mujeres en relaciones de pareja (VcM, constructo o variable independiente) afecta a la productividad laboral (constructo o variable dependiente) medido en terminos de ausentismo y presentismo, y la explicacion de esta relacion es por medio del dano a la salud mental y fisica (constructo o variable mediadora). Para ello, se entrevistaron a 357 duenas de microempresas formales en 10 departamentos del Peru y a 977 duenas de microempresas informales o formales con acceso a credito en Paraguay, se aplico un cuestionario estructurado cuyas preguntas estuvieron medidas en escala ordinal. En el modelo de media, los resultados del PLS-SEM muestran que tanto en Peru y Paraguay los tres constructos analizados son validos y confiables, fundamentados por la fiabilidad compuesta, las cargas de los indicadores, la varianza extraida media (AVE), las cargas cruzadas, el criterios de Fornell Larcker y el Heterorrasgo-Monorrasgo (HTMT). En cuanto al modelo estructural, tanto en Peru como en Paraguay los hallazgos proporcionan evidencia empirica de que el dano a la salud mental y fisica explica la relacion entre VcM y productividad laboral. Por consiguiente, la presente investigación posee implicancias por la confirmacion del efecto mediador, la aplicacion de esta tecnica en este tipo de tematica y el desarrollo del marco teorico y practico del PLS-SEM.",Universidad Nacional Agraria La Molina,Influencia de la violencia contra las mujeres en la productividad laboral de microempresas utilizando ecuaciones estructurales con mínimos cuadrados parciales,Magister Scientiae - Estadística Aplicada
"Sotomayor Ruíz, Rino Nicanor","Castillo Gamarra, Jorge Enrique",2014,"El presente trabajo tiene como objetivo describir los modelos de varianza condicional ARCH y GARCH junto con sus propiedades y demostraciones, estos modelos se aplican en series de tiempo financieras, debido a que estas presentan como característica principal una fuerte volatilidad con periodos de calma o agitación, lo cual no permite utilizar los modelos de series de tiempo tradicionales que asumen varianzas constantes. Así mismo se realizó una aplicación utilizando como variable el valor diario del Índice General de la Bolsa de Valores de Lima (IGBVL), periodo 2009 – 2011, para la aplicación  se utilizó el software econométrico Eviews 7. Al analizar los resultados de las estimaciones de los modelos que explicarían la volatilidad diaria de la Rentabilidad del Índice General de la Bolsa de Valores (RIGBVL), periodo 2009 – 2011, se  concluyó que  el  modelo GARCH (1,1) es el adecuado, debido a que el modelo GARCH (1,1) tiene a diferencia de los demás modelos el menor valor tanto en el criterio de información de Akaike (AIC) como en el criterio de información de Schwarz. Previamente se modeló la media de la RIGVBL con el modelo AR (1)",Universidad Nacional Agraria La Molina,"Modelación de la volatilidad del índice general de la Bolsa de Valores de Lima, periodo 2009-2011",Ingeniero Estadístico e Informático
"Chue Gallardo, Jorge","Meza Rodríguez, Aldo Richard",2018,"La presente investigación tiene como propósito aplicar y comparar el modelo de regresión logística y el algoritmo Adaboost en datos desbalanceados, esto a efecto de predecir la fuga  de clientes en una empresa del sector de telefonía móvil. El algoritmo Adaboost se sustenta  en el aprendizaje adaptativo al entrenar clasificadores débiles combinándolos en conjunto  para obtener un clasificador cuyo rendimiento sea fuerte. En cuanto a la regresión logística su modelamiento se realizó estrictamente desde una perspectiva de minería de datos, donde la clasificación es el objetivo y el rendimiento se evaluó en un conjunto de validación. Ambas técnicas se compararon mediante dos procedimientos, el primero mediante métodos de muestreo (sub-muestreo, sobre-muestreo y SMOTE) y el segundo modificando y/o  ajustando el algoritmo o función. Al trabajar con datos desbalanceados la tasa de error de clasificación es ineficiente, por lo que las medidas de desempeño para elegir al mejor modelo fueron la precisión, el recall (sensibilidad), el F-measure, y como medida principal el AUC a través de curvas ROC. Al formar modelos logísticos con los métodos de muestreo, las medidas de desempeño arrojaron resultados similares, lo mismo pasó al formar modelos con el algoritmo Adaboost, sin embargo al comparar la regresión logística (AUC=0.86) con el algoritmo Adaboost (AUC =0.93), este último tuvo el mejor desempeño. En cuanto al ajuste a nivel de algoritmo o función, en la regresión logística se trabajó de dos maneras, el primero (Logit Asym) incluyendo en la FDA un valor Kappa (k) y el segundo (Power Logit) un valor Lambda (λ), en ambos modelos se identificaron los valores óptimos de k (0.02) y λ (2.5), en cuanto al algoritmo Adaboost (Adaboost Asym) se ajustó el peso de la clase minoritaria cuyo costo de clasificación fue errónea. La comparación de estos tres modelos ajustados dio como mayor rendimiento al algoritmo Adaboost. Finalmente se realizó la validación cruzada con 10 iteraciones para todos los modelos dando resultados similares al método de retención. Realizada todas las comparaciones y las medidas de desempeño se concluye que el modelo óptimo para la predicción de fuga de clientes en la empresa de telefonía es el algoritmo Adaboost",Universidad Nacional Agraria La Molina,Predicción de fuga de clientes en una empresa de telefonía utilizando el algoritmo Adaboost desbalanceado y la regresión logística asimétrica,Magister Scientiae - Estadística Aplicada
"Miranda Villagomez, Clodomiro Fernando","Chafloque Cespedes, María Raquel",2019,"Los Modelos de Ecuaciones Estructurales con Mínimos Cuadrados Parciales (PLS – SEM, por sus siglas en inglés), son un método de segunda generación, con gran aceptación en la actualidad en el mundo académico, en especial en el área de ciencias empresariales. Asimismo, el enfoque de esta técnica es más robusto y flexible al momento de utilizarlo en variables no observables. La presente investigación muestra una aplicación del PLS – SEM dentro del área de ciencias empresariales, en un sector económico donde no existe evidencia empírica cuando se habla de estrategias de marketing y desempeño empresarial. La investigación tuvo como objetivo determinar la relación entre la orientación de mercado, la innovación del producto y el desempeño de las empresas en el sector artesanal peruano - periodo 2018 mediante la aplicación del PLS-SEM. Se aplicó una encuesta estructurada a 301 microempresas del sector artesanal, específicamente las que están en el rubro comercial, siendo estas las principales intermediarias entre el consumidor final y el productor. Se encontró que la orientación de mercado y la innovación del producto explican el 34.3% del desempeño de la empresa; así mismo la orientación de mercado se relaciona positivamente a la innovación del producto, y esta última variable se relaciona positivamente al desempeño de la empresa. Se concluye que la técnica de PLS – SEM es adecuada para ser aplicada a investigaciones de ciencias empresariales. Finalmente, se recomienda que se incremente la evidencia empírica con el fin de fomentar el uso de esta técnica estadística de segunda generación.",Universidad Nacional Agraria La Molina,Aplicación de los modelos de ecuaciones estructurales a las empresas del sector artesanal peruano,Magister Scientiae - Estadística Aplicada
"Sotomayor Ruíz, Rino Nicanor","Loaiza Alamo, Marco Antonio",2015,"El objetivo de esta investigación es determinar la mejor selección de los siete programas especializados para la implementación de un laboratorio mediante la aplicación del Análisis de Proceso Jerárquico (AHP). Con esta técnica se logró un consenso para identificar cuáles son los criterios y las alternativas más relevantes para la toma de decisiones. Para validar el AHP se necesitó los Índices de Consistencia: el Cociente de Resistencia (CR) para la matriz de comparación de criterios y alternativas por pares, el Índice de Consistencia Geométrica (GCI) y el Indicador de Consenso AHP (S*) para la matriz consolidada. Con el desarrollo del algoritmo de AHP se optó por cinco criterios y cuatro alternativas para la toma de decisión. Se concluyó que la alternativa A, conformada por los programas informáticos Minitab, SPSS, SQL, Eviews y Microsoft Project, fue la más importante. No obstante, no hubo diferencia significativa considerable con la alternativa C, conformada por los programas informáticos Minitab, SPSS, SQL, Eviews y QlikView. En base a los resultados obtenidos se concluye que los programas más adecuados para la implementación del laboratorio informático son: Minitab, SPSS, SQL, Eviews, Microsoft Proyect y QlikView",Universidad Nacional Agraria La Molina,Uso del criterio AHP para la toma de decisiones,Ingeniero Estadístico e Informático
,"Mamani Tone, Edith Rita",2017,"La presente monografía estudia la Descripción Metodológica del Modelo de Ecuaciones Estructurales con el Método de Estimación de Mínimos Cuadrados Parciales. En el primer capitulo se describe detalladamente la sustentación teórica del Modelo de Ecuaciones Estructurales; en el capítulo 2 se describe el Método de Mínimos Cuadrados Parciales (PLS); en capítulo 3 se describe la Metodología detallando los pasos del Modelo de Ecuaciones Estructurales por el Método de Mínimos Cuadrados Parciales y en el capítulo 4 se observa la Aplicación, paso a paso en un caso para ejemplificar la metodología. Se concluyó los modelos de ecuaciones estructurales SEM es una extensión de la regresión múltiple, se aplica esta técnica para encontrar relaciones entre variables observables y no observables llamadas (latentes) para pasar posteriormente a estimar los parámetros. tiene como objetivo la predicción, no es preciso que los datos provengan de una distribución normal y puede aplicarse a estudios de muestras pequeñas, permite estimar modelos muy complejos con muchas variables latentes y medibles. En la aplicación de la descripción metodológica se realizó un estudio empírico el segundo semestre de 2013 sobre una muestra correspondiente a 300 alumnos universitarios chilenos con acceso a bases de datos científicas. Para los cálculos de PLS se utilizó el software WarpPLS 4.0. Los resultados del ejemplo en el análisis de PLS del caso indicaron la buena capacidad predictiva del modelo de investigación, y a su vez, la explicación del análisis logró ejemplificar en forma clara la metodología propuesta.",Universidad Nacional Agraria La Molina,Descripción metodológica del modelo de ecuaciones estructurales con el método de estimación de mínimos cuadrados parciales,Ingeniero Estadístico e Informático
"Salinas Flores, Jesús Walter","Elguera Vega, Rhony Miguel",2018,"En la actualidad, la gran cantidad de datos que se almacenan de los clientes en las diferentes empresas y la capacidad de procesamiento que brindan las computadoras, han generado gran interés por investigar; así como, desarrollar métodos y algoritmos para el análisis de agrupamiento. Los métodos de agrupamiento dirigidos a la segmentación de clientes permiten a las empresas identificar los patrones y perfiles de compra o servicios, ayudando a tomar mejores decisiones de las estrategias de canales y publicidad para sus clientes. En la presente investigación se aplica el método de agrupamiento basado en las particiones de k-Medoides con el algoritmo PAM (Partición Alrededor de Medoides). El algoritmo PAM se basa en particionar el conjunto de datos en k grupos, donde k es conocido; es considerado más robusto ante datos atípicos y el ruido, se basa en minimizar la suma de disimilitudes entre un objeto y el Medoide (centro del grupo). El objetivo de la presente investigación es aplicar el algoritmo PAM para segmentar a los clientes de un casino con los datos obtenidos, a través del uso de tarjetas en el tragamonedas. El método de la silueta permitió identificar tres clústers como el número óptimo. El análisis de agrupamiento con el algoritmo PAM usando la medida de distancia Gower, resultó la segmentación de clientes para los tres clúster con porcentajes de 49.4%, 11.3% y 39.4% respectivamente. La agrupación fue validada, al obtener para las 6 variables cuantitativas todos los ANVAs significativos y con el árbol de clasificación C5.0 un 99.35% de precisión.  Los resultados de la caracterización muestran que el clúster 1 son clientes con valores de los promedios para las 6 variables en un nivel intermedio, el 67.0% son hombres y 100% el tipo de tarjeta es classic. En el clúster 2 están los clientes con los valores más altos en los promedio de las 6 variables, el 59% son hombres y el 100% usan la tarjeta silver. En el clúster 3, se encuentran los clientes con los promedios más bajos, el 64% son hombres y el 100% usan tarjeta classic",Universidad Nacional Agraria La Molina,Segmentación de clientes de un casino utilizando el algoritmo partición alrededor de medoides (PAM) con datos mixtos,Ingeniero Estadístico e Informático
"Miranda Villagomez, Clodomiro Fernando||Acevedo Mallque, Moisés Pascual","Montenegro Muro, Rolando Antonio",2018,"El bosque amazónico cuenta con una gran variedad de especies arbóreas, la cual se estima en cuatro mil especies. Unas pocas especies amazónicas enfrentan la desaparición por la alta demanda de su madera. Para reducir la presión sobre las especies demandadas y promover el uso de nuevas especies es necesario conocer sus características tecnológicas, principalmente, las características físico mecánicas. Debido a la gran cantidad de propiedades que encierran dichas características, se propuso estudiar a las especies con técnicas multivariadas, específicamente a través del análisis de conglomerados. Ello con la finalidad de agruparlas en función a la similitud que tengan en sus propiedades físicas y mecánicas. Así, se pueden agrupar especies poco conocidas en el mercado con especies muy demandadas y sugerir potenciales usos. Para el estudio presente se utilizó el algoritmo CLARA (Clustering Large Applications), el cual es empleado en grandes conjuntos de datos. Para seleccionar el número de conglomerados óptimo se probó hacer de dos hasta diez grupos; luego se comparó el ancho de la silueta promedio y el índice de Dunn por grupo y se eligió el de valores más altos. Se encontró que con un ancho de la Silueta promedio de 0,339 el número óptimo de conglomerados es de dos. El número de conglomerados indicado coincide con el análisis realizado a partir del índice de Dunn, el cual alcanza su más alto valor en 0,1264 con dos clústeres. Los conglomerados tuvieron como medóides a Guarea subridiflora (“requia de altura”) y Retrophyllum tospigliosii (“ulcumano). El primer conglomerado se caracterizó por tener propiedades mecánicas y físicas altas, de acuerdo a lo establecido por Aróstegui et al (1986). Por otro lado, el conglomerado de medóide “ulcumano” se caracterizó por tener propiedades físico mecánicas bajas, a excepción del clivaje, el cual resultó ser medio.",Universidad Nacional Agraria La Molina,Clasificación de especies forestales maderables de la Amazonía Peruana aplicando análisis Clúster con algoritmo Clara,Magister Scientiae - Estadística Aplicada
"Chue Gallardo, Jorge","Valdivia Carbajal, Manuel",2019,"Esta tesis toma como caso de estudio a una empresa de cosméticos reconocida de la ciudad de Lima, Perú. Para pronosticar el riesgo de crédito se analizaron dos modelos: la Regresión Binaria Asimétrica Cloglog y las Redes Neuronales Artificiales Perceptrón Multicapa. La selección de estos modelos surge a raíz de recientes estudios que revelan las ventajas de las técnicas de inteligencia artificial sobre los modelos estadísticos en cuanto a predicción por su alta capacidad de discernimiento de patrones. “La empresa” cuenta con un modelo de negocio llamado Red Binaria, esto quiere decir que se contrata vendedoras y éstas ofrecen productos a sus clientes a través de catálogos. Debido a que no se cuenta con información de los clientes finales, se midió la probabilidad de no pago a través de las vendedoras. La población de estudio estuvo conformada por las vendedoras de la empresa las cuales manejan una cartera de clientes de 51183 personas a julio del 2017. Los datos se trataron previamente considerando el análisis de valores atípicos a nivel univariado y multivariado, este último mediante el algoritmo de segmentación K-means. Concluido ello para realizar la clasificación de vendedoras en buenas y malas pagadoras se utilizó un modelo de Redes Neuronales Artificiales Perceptrón Multicapa con una sola capa intermedia y un modelo de regresión Binaria sobre el cual se eligió el enlace asimétrico Cloglog debido a la naturaleza de los datos. Los resultados mostraron un 0.846 y 0.809 de índice ROC en las muestras de entrenamiento, y un 0.762 y 0.733 de índice ROC en las muestras de testeo respectivamente para cada modelo. Finalmente, se concluye que la aplicación de la técnica de Redes Neuronales Perceptrón Multicapa define una mejor regla de discriminación que la Regresión Binaria Asimétrica Cloglog en el estudio de probabilidad de impago. Además, las Redes Neuronales presentan mejores indicadores de pronóstico.",Universidad Nacional Agraria La Molina,Comparación del pronóstico de riesgo de crédito utilizando regresión binaria asimétrica cloglog y perceptrón multicapa,Magister Scientiae - Estadística Aplicada
"Miranda Villagomez, Clodomiro Fernando","Málaga Sabogal, Lucía",2017,"Este estudio halló los grupos de investigación conformados por las instituciones peruanas con investigación en medicina indizada en Scopus en base a las coautorías. La información se descargó de Scopus en formato no estandarizado y se utilizó aprendizaje supervisado con k-medias y un conjunto de datos de entrenamiento, para la identificación de las instituciones involucradas. El procesamiento de los datos se hizo con R. Las instituciones identificadas se clasificaron en ocho categorías: universidades, institutos públicos de investigación, clínicas y hospitales, organismos y dependencias del gobierno nacional, organismos y dependencias del gobierno local, empresas, organizaciones internacionales con filiales en Perú, instituciones privadas sin fines de lucro; y dos sectores: público y privado. Posteriormente se identificó los conglomerados existentes utilizando la metodología de particionamiento jerárquico aglomerativo propuesta por Moore, Clauset y Newman e implementada en el paquete igraph en R. Se halló que las instituciones del sector salud tienden a colaborar con sus símiles pero que no existe relación entre el tipo y sector de la institución y los patrones de colaboración para otras instituciones",Universidad Nacional Agraria La Molina,Identificación de conglomerados en el grado de coautorias formado por las instituciones peruanas con investigación en medicina indizada en Scopus,Magister Scientiae - Estadística Aplicada
"Maehara Oyata, Víctor Manuel","Vargas Paredes, Ana Cecilia",2017,"Se estimó mediante un modelo lineal mixto los componentes de varianza y heredabilidad de la producción de leche, a partir de los registros de 3397 lactaciones, provenientes de 1359 vacas de raza Holsteins, de 57 rebaños con información genealógica de 5 generaciones, utilizando máxima verosimilitud restringida conocida como REML y muestreo de Gibbs basado en procedimientos bayesianos. Con ambas metodologías se obtuvo una heredabilidad, en sentido amplio, moderada de 0.135 vía REML y una media de 0.318 vía muestreo de Gibbs. Para realizar el análisis exploratorio de residuales (en función de los tres tipos: marginal, residual condicional y efectos aleatorios) del modelo lineal mixto estimado vía REML, se adaptó funciones en R para incorporar la información genealógica o pedigrí al modelo. Como resultado de esto se verificó la linealidad de los efectos fijos y la normalidad del componente genético del animal. No se encontró normalidad para el efecto aleatorio del rebaño ni para los residuales condicionales. Para  estos últimos tampoco se observó homocedasticidad. Además, se encontró que para 132 animales la estructura de covarianza considerada en el modelo no es adecuada. También, se observó 215 animales y 7 rebaños con efectos atípicos. En el diagnóstico del procedimiento de simulación del muestreo de Gibbs desde la perspectiva bayesiana no se encontró problemas de convergencia. Se obtuvieron errores de Montecarlo bajos y tamaños efectivos de muestra mayores a 1000 para cada componente del modelo.",Universidad Nacional Agraria La Molina,Estimación de componentes de varianza utilizando los métodos bayesianos y máxima verosimilitud restringida para el estudio de la heredabilidad,Magister Scientiae - Estadística Aplicada
"Gonzáles Chavesta, Celso","Recuay Denegri, Briggitt Giuliana",2017,"Ante un mercado cambiante las empresas del rubro de consumo masivo se encuentran sumergidas en una serie de interrogantes en cuanto a la participación de sus productos. Para ello aplican herramientas como la auditoría de mercado que a través del resultado de una de sus etapas el “análisis de datos” encuentran confianza a la hora de tomar decisiones. Este estudio presenta el proceso del análisis de datos en una auditoría de mercado para productos de consumo masivo en el canal tradicional. Los productos analizados fueron las cremas dentales, los desodorantes y los energizantes. Se trabajó con una muestra de 1200 bodegas ubicadas dentro de Lima Metropolitana. El conjunto de datos contiene variables cuantitativas como las ventas, las compras, los precios y los inventarios, además de las variables cualitativas como son los atributos (marca, tamaño, sabor, etc.) de las variedades. Los datos fueron recolectados de las bodegas el primer semestre del año 2016 vía celular mediante el aplicativo de relevamiento “Audit” para luego ser cargados al sistema de trabajo. Culminada cada fase del análisis de datos (limpieza de datos, análisis de las variables y visualización de resultados) se supervisaron las bodegas cuyas variedades presentaron incoherencias en sus datos y se procedió a corregir. Para los tres productos en su mayoría se encontraron: ausencia de datos en la variable compra e inventario y errores en los precios y ventas. Estos errores suelen suceder debido a una mala digitación (error de codificación) o a la omisión de parte del supervisor al momento de ingresar los datos al aplicativo. Como resultado del estudio se obtuvieron datos consistentes para las cremas dentales, desodorantes y energizantes, los cuales fueron de mucha relevancia para las empresas. Otros productos de consumo masivo pueden seguir también la misma estructura para el análisis de sus datos.",Universidad Nacional Agraria La Molina,Análisis de datos en una auditoría de mercado para productos de consumo masivo en bodegas de Lima Metropolitana,Ingeniero Estadístico e Informático
"Sotomayor Ruíz, Rino Nicanor","Chumpitaz Ramos, Domingo Guzmán",2021,"Los Modelos de Ecuaciones Estructurales (SEM), es una extensión de varias técnicas multivariantes, entre ellas el Análisis Factorial, se ha utilizado casi en todos los campos de estudio, principalmente en el área de la educación, esta técnica nos proporciona un método directo para tratar múltiples relaciones de variables observables y no observables. El objetivo principal de la investigación fue determinar la relación entre la calidad de servicios con los factores que la determinan (tales como la seguridad, fiabilidad, empatía, aspectos tangibles y capacidad de respuesta); y la satisfacción estudiantil universitaria utilizando Modelos de Ecuaciones Estructurales (SEM). En la investigación se diseñó un cuestionario en base al instrumento de calidad de servicio SERVQUAL relacionado a un modelo estructural teórico del autor Lobos y Sepúlveda. Se hizo un análisis de fiabilidad del cuestionario, y con el análisis factorial exploratorio se demostró su validez. Luego, se recolectó información de 158 estudiantes del Área de Ciencias Económicas y de la Gestión de la UNMSM. Para desarrollar la investigación se aplicó el Análisis Factorial Confirmatorio (AFC) y Modelos de Ecuaciones Estructurales (SEM). En el modelo estructural inicial, los constructos empatía y fiabilidad no fueron significativos, entonces se reespecificó el modelo con los índices de modificación. En el modelo estructural 2, las variables observadas X5, X6 y X12 no fueron significativas en los constructos de fiabilidad y empatía, por lo tanto se retiraron del modelo. En el modelo estructural reespecificado 3, empatía no fue significativo, entonces no fue considerado en el siguiente modelo. En el modelo estructural final los constructos de seguridad, fiabilidad y aspectos tangibles fueron significativos con un nivel de significancia de 0.05. La calidad de servicio esperada tiene una relación directa con la satisfacción estudiantil y, los constructos de seguridad, fiabilidad y aspectos tangibles están relacionados positivamente con la calidad de servicio esperada.",Universidad Nacional Agraria La Molina,Factores que determinan la calidad de servicio y su relación con la satisfacción estudiantil universitaria estatal utilizando ecuaciones estructurales,Magister Scientiae - Estadística Aplicada
"Menacho Chiok, César Higinio","Alarcón Pimentel, Sandra Elena",2021,"El sector asegurador peruano es supervisado en todo momento por la Superintendencia de Banca y Seguros – SBS. Esta entidad vigila y evalúa el comportamiento, prácticas e información relacionada a las aseguradoras con el objetivo de fomentar la rentabilidad, transparencia y una mayor protección de asegurados y beneficiarios. Para lograrlo las aseguradoras deben manejar métodos de estimación de provisiones (reservas) técnicas que contengan fundamentos estadísticos y que los resultados obtenidos sean lo más certero posible. En esta monografía, se presenta la metodología y la aplicación del método estocástico Double Chain Ladder - DCL para el Seguro Obligatorio de Accidentes de Tránsito – SOAT, siendo este tipo de método el más ventajoso para la estimación de pagos futuros de siniestros. Los resultados con la metodología DCL para estimación de la provisión de pagos futuros de los accidentes en el SOAT para la empresa aseguradora, se obtuvieron valores estimados para las reservas de siniestros ocurridos (RBNS) de S/. 293,206 y para las reservas de siniestros ocurridos y no reportados (IBNYR) de S/. 48,826 y para el total de las  reservas IBNR en S/. 342,033. En comparación, con el método Double Chain Ladder se obtuvo una reserva de S/. 342,033 menor que con el método Chain Ladder S/349,117. En la repartición de los siniestros, el 90% es de las reservas son RBNS y un 10% de las IBNYR, lo que implica que la gran mayoría de siniestros ya han sido notificados a la compañía, pero hay un retraso en la liquidación",Universidad Nacional Agraria La Molina,Estimación del monto de siniestros ocurridos y no reportados para el SOAT con el método Double Cchain Ladder,Ingeniero Estadístico e Informático
"Gamboa Unsihhuay, Jesús Eduardo","Romero Cuadros, Italo Brayan",2022,"Esta investigación tiene como objetivo de determinar los patrones de clasificación del desempeño del sector público mediante la técnica de análisis conglomerados de series de tiempo a las regiones del Perú para el periodo 2007 – 2019. El desarrollo se realizó con los indicadores creados bajo la metodología de la frontera de posibilidades de producción y se evaluó la asociación a través del tiempo con el método de distancia de deformación del tiempo. El indicador utilizado para decidir el número de los conglomerados fue el de Silueta y Calinski. Entre los resultados más importantes se encontró que se mantiene un patrón que predomina entre las regiones que mejor usan los recursos y obtienen resultados idóneos donde resalta Moquegua, Ica y Lima mientras que otro grupo que mantiene fuertes tendencias a ser ineficientes en comparación como Ayacucho y Huancavelica. Finalmente, se observó una predominancia en las regiones que subdivide entre dos conglomerados y se mantiene tanto sectorial y global.",Universidad Nacional Agraria La Molina,"Clasificación de la eficiencia del gasto público en las regiones del Perú aplicando conglomerados de series temporales, 2007 - 2019",Magister Scientiae - Estadística Aplicada
"Salinas Flores, Jesús Walter","Marcos Sánchez, Eduardo Angelo",2015,"El crecimiento de los créditos por consumo a nivel mundial, junto con la normativa internacional sobre requerimientos de capital (Basilea II), están impulsando a las instituciones de banca retail a una mayor competencia con las entidades bancarias por este segmento de negocio. Por lo tanto en la empresa Carsa S.A.C. tiende a utilizar estrategias de ventas a crédito para tener un mayor crecimiento en este mercado competitivo, sin embargo su falta de control e identificación de clientes buenos y malos hace que sus estrategias puedan causar grandes pérdidas. Por ello, el objetivo de investigación del estudio es proponer un modelo de regresión logística, con el fin de analizar el riesgo crediticio en las ventas al minoreo en la empresa Carsa S. A. C. La investigación es de tipo descriptiva, explicativa transversal ya que comprende una población de los créditos generados en el periodo de enero y noviembre del año 2013 de electrodomésticos vendidos a nivel nacional. El modelo de regresión logística encontrado está compuesto por las variables: tipo de actividad, línea del producto, tipo de propiedad, estado civil, total de créditos, productos crediticios, plazo del crédito y mora máximo en el sistema financiero. En consecuencia, la empresa Carsa S. A. C. debe incluir el modelo de credit scoring en su política de crédito como un filtro adicional al otorgamiento convencional del mismo, dicho modelo es el cual discrimina a los clientes buenos y malos basándose en el perfil del cliente previo al otorgamiento de crédito. En la actualidad, la empresa analizada tiene grandes pérdidas por clientes morosos o mal identificados, efecto causado por el otorgamiento del crédito por el método por experiencia. Las pérdidas monetarias de solicitudes que debieron ser rechazadas en el 2013 ascienden a S/. 658,000.00 nuevos soles.",Universidad Nacional Agraria La Molina,Propuesta de un modelo de regresión logística para analizar el riesgo crediticio en la empresa CARSA S.A.C.,Ingeniero Estadístico e Informático
"Sotomayor Ruíz, Rino Nicanor","Prado Pariona, Vanesa||Palomino Quispe, Jacqueline Roxana",2015,"El trabajo, consistió en detallar paso a paso la metodología (CRISP–DM) para poder identificar grupos óptimos de clientes más propensos a migrar de un plan prepago a postpago con el fin de formular un plan de mejora en la gestión de llamadas mediante la clasificación de la base de datos. Este trabajo ha sido motivado por que actualmente se ha visto una disminución de la tasa de efectividad y contactabilidad con los clientes, para esto se ha utilizado el software Rapid Miner ya que es más detallada la representación de flujos de manera gráfica y por su gran capacidad para trabajar con una amplia gama de bases de datos. Se aplicaron modelos de clasificación para analizar las características que genera la compra de los diferentes servicios. Se realizó la comparación del modelo de Regresión Logística y el algoritmo de Árbol de Clasificación CART, quedando como modelo más óptimo la Regresión Logística ya que ofreció mejores resultados y mayor efectividad. A partir de lo anterior, se encontraron grupos diferenciados por las probabilidades de éxito venta (Migrar de un plan prepago a postpago), segmentos que reflejan necesidades y características particulares, que permita diseñar acciones de marketing focalizado con el objetivo de incrementar la tasa de efectividad, contactabilidad e incrementar las ventas. Se realizaron recomendaciones para futuras acciones de marketing, un ejemplo es identificar grupos que se debe intentar desarrollar y otros grupos que sólo que se debe tratar de fidelizar, ya que han alcanzado gran parte de su potencial dentro de la empresa. Cómo trabajos futuros se recomienda replicar la metodología con mayor información demográfica, con el fin de aumentar los índices de desempeño de los modelos predictivos. Además de poder cuantificar el aumento de la efectividad debido a la aplicación de esta metodología, a través de una campaña real.",Universidad Nacional Agraria La Molina,"Segmentación de la base de datos de un call center para las ventas del servicio de telefonía móvil, usando el modelo de regresión logística y el algoritmo de árbol de clasificación CART",Ingeniero Estadístico e Informático
"Sotomayor Ruíz, Rino Nicanor","Bernabé Ponte, Eduardo||Ballón Beltrán, Dante Daniel",2015,"En el presente estudio se tiene como objetivo primordial describir y predecir la vía de culminación de parto en gestantes (Normal o por cesárea) mediante un modelo logístico binario y como objetivo secundario identificar los factores asociados que permiten determinar si una gestante tendrá una vía de culminación de parto: normal o por cesárea. Luego se realizará el análisis de regresión logística binaria con el fin de encontrar el mejor modelo que explique la variable de interés en función a las siguientes variables independientes: complicaciones de la madre, edad de la madre, edad gestacional, número de bebes, peso del bebe por nacer, talla del bebe por nacer y perímetro cefálico  del bebe por nacer. Los principales resultados que presentamos son: Tasa de clasificación según vía de culminación del parto en gestantes 88.5%; el coeficiente de determinación del modelo 65.9%, pruebas estadísticas de Hosmer y Lemeshow 0.760, lo cual verifica que nuestros datos se ajustan al modelo logístico binario, la tasa de mala clasificación del modelo validada mediante validación cruzada con 11.65%, siendo un error de estimación bajo y aceptable.",Universidad Nacional Agraria La Molina,Análisis clasificatorio de las gestantes según vía de culminación del parto aplicando regresión logística binaria,Ingeniero Estadístico e Informático
"Salinas Flores, Jesús Walter","Vizcarra Mac, César Augusto||Lajo Salazar, Julian Javier",2016,"En el siguiente trabajo se busca resolver un problema que ocurre a menudo en diversas empresas, especialmente dentro del área de marketing, mediante el uso de tres técnicas estadísticas. Aplicándolas adecuadamente se encontraron nichos de usuarios del servicio de telefonía móvil, a los cuales se les identificaron perfiles que los describieran adecuadamente y finalmente se buscaron que estas agrupaciones contribuyan a obtener un nivel de eficiencia mayor al realizar alguna acción de marketing específica. Durante el análisis se encontraron tres grupos de personas, estos grupos ayudarán a realizar un marketing mejor enfocado a cada tipo de cliente y así obtener mejores resultados, ya sea en venta como en inversión publicitaria. La optimización de estos recursos servirá a las empresas de servicio de telefonía móvil para poder llegar a los usuarios de una manera más directa en lugar de realizar acciones de marketing masivo los cuales muchas veces son poco rentables. El primer grupo que se encontró es el de los TRADICIONALES. Son aquellas personas que les importan poco tener lo último en tecnología pero buscan que su teléfono móvil funcione como debe ser y les permita algo más que llamar y enviar mensajes. Valoran que los demás los reconozcan por usarlo pero prestan poca atención sobre la marca que usan. El segundo grupo que se encontró es el de los MODERNOS. Ellos valoran que su equipo luzca moderno y sea de una marca reconocida. Quieren algo que les dure mucho tiempo, pero conocen muy poco o nada sobre todas las funcionalidades. En muchos casos no saben usarlas. Además, les importa mucho ser reconocidos por las características físicas exteriores de sus teléfonos móviles. Por último se tiene al grupo de los TECNOLÓGICOS. Ellos no solo buscan un teléfono móvil tecnológico acorde a sus necesidades sino que también saben aprovechar sus funcionalidades (conocen bastante sobre tecnología). Ser reconocidos por lo que usan es algo adicional pero no primordial, valoran mucho más la calidad y seguridad que una marca reconocida les puede brindar. Una vez descritos estos grupos de usuarios, las empresas que empiecen a realizar acciones de marketing específicas para cada grupo lograrán estar por delante de la competencia ya que al tener una comprensión más profunda de cómo es que piensan los usuarios de telefonía móvil podrán generar estrategias más directas acorde a las necesidades del consumidor.",Universidad Nacional Agraria La Molina,Análisis de preferencia de servicios de telefonía móvil para segmentar a los clientes que usan smartphone,Ingeniero Estadístico e Informático
"Gonzáles Chavesta, Celso","Yopán Comeca, Herbert Alfonso||Gálvez Castillo, Renato Martín",2014,"El trabajo nace con la necesidad de mejorar el proceso de la confección de las prendas de vestir el cual origina un alto porcentaje de prendas de segunda calidad generando así grandes pérdidas económicas (US$ 100,000 aprox. al año) y dañando la imagen de la empresa con sus clientes los cuales en su mayoría son de Europa, EEUU y Canadá. Hay que tener siempre en cuenta que los clientes establecen un máximo permitido de prendas defectuosas por pedido, un porcentaje mayor conlleva a penalidades y una baja puntuación por parte de los clientes al momento de las encuestas de satisfacción.  Por tal motivo la empresa implementó la metodología Seis Sigma para ofrecer un mejor producto o servicio al cliente, de una manera más rápida y al costo más bajo posible, la cual se inició el 04 de Marzo del 2013 y finalizó el 26 de Julio del 2013. Se empezó con una breve descripción del proceso de confección de prendas de vestir y las operaciones en el que está involucrado; luego se ejecutó el ciclo DMAIC: Definir, Medir, Analizar, Implementar (Mejorar) y Controlar. En la fase de Definición; se identificó las necesidades del cliente y los requerimientos críticos del producto (medidas de la prenda, costuras, modelo solicitado y doblado para despacho), como también de los principales defectos que generan prendas de segunda calidad, por lo que se utilizó diagramas SIPOC y gráficos de Pareto. En la fase de Medición; se identificó y cuantificó las variables más relevantes del proceso de confección de prendas a controlar. Mediante el uso de herramientas de calidad como el estudio Gage R&R, gráficos de control, análisis de la capacidad del proceso y el nivel Seis Sigma se obtuvo la situación actual por la que atraviesa el proceso en estudio. El porcentaje promedio de productos no conformes por  prendas de segunda calidad es del 0.90% con un nivel Six Sigma de 0.89 y un CPk de -0.20 lo cual nos indica que el proceso no es capaz de cumplir con los requerimientos del cliente, esta incapacidad del proceso se refleja en el alto porcentaje de piezas defectuosas por millón (PPM) que es de 72.92%.En la fase de Análisis; se identificó las causas raíces que originan el problema, para ello se aplicó análisis de correlación, análisis de regresión múltiple y prueba de hipótesis que ayudaron a identificar a la polivalencia de los costureros, el estado de las máquinas de coser, el número de la muestra de costura y la calidad de agujas como los factores mas importantes que afectan al porcentaje de prendas de segunda calidad. En la fase de Mejora, se ejecutó las acciones en base a los resultados obtenidos en la fase de análisis con el objetivo de optimizar el proceso de confección de prendas de vestir; se plantean planes de acción; donde se ejecutan programas de capacitación al personal y se elaboran instructivos de trabajo para el proceso de costura. La fase de Control; es la última de las fases y busca mantener los resultados obtenidos en la fase de mejora, para tal efecto calculamos el nuevo promedio y nivel Seis Sigma, obteniéndose los valores de 0.59% de prendas de segunda calidad y 3.89 nivel sigma respectivamente reduciendo significativamente el porcentaje de piezas defectuosas por millón (PPM) a 0.84%. Para tener bajo control el indicador y asegurar que mejore el tiempo, se diseñó planes de capacitaciones continuas a los 3 niveles operativos de la planta (operarios, supervisores, jefaturas). Es así que se asegura que el porcentaje de prendas de segunda calidad por defectos de costura se mantenga dentro del intervalo planteado.   Finalmente; en la evaluación económica realizada podemos notar claramente la reducción significativa de los costos de calidad, reduciéndose en promedio de S/. 24, 830  a S/. 10,100 al mes; esto significa una reducción del 60% de los costos de calidad. Si súmanos a esto la mejora de la imagen de la empresa entre sus clientes reflejadas en la disminución de penalidades por productos no conformes y una mayor puntuación en las encuestas al final de cada semestre el beneficio es aún mayor. En consecuencia, la ejecución del proyecto utilizando la metodología Seis Sigma ayudó a obtener beneficios económicos esperados.",Universidad Nacional Agraria La Molina,"Reducción del porcentaje de prensas de segunda por defectos de confección utilizando la metodología Seis Sigma, caso: empresa Textil-Confecciones",Ingeniero Estadístico e Informático
"Vega Huerta, Hugo Froilán","Salas Arbaiza, Cesar Enrique",2023,"Valida un modelo de aceptación tecnológica, que fomente el incremento de los indicadores financieros de las empresas del SEMA. La investigación, se centra en la etapa de lanzamiento o puesta en producción del ciclo de vida de desarrollo de software, siendo necesario gestionar un proceso de adopción de tecnología para una adecuada transición a la nueva tecnología. La herramienta tecnológica usada en el proceso de adopción es la Ventanilla Única de Comercio Exterior (VUCE) y se aplicó al Sector Exportador de Mercancías Agrícolas del Perú (SEMA) la aceptación será más conveniente si evidencia incremento de las ventas y operaciones comerciales en el sector. Para esta investigación, se relevó los factores importantes y necesarios para una conveniente adopción tecnológica, se revisó el estado del arte de los Modelo de Aceptación Tecnológica (TAM) del 2001 al 2019,
encontrando 10 modelos y 36 factores. Se plantea adaptar el modelo TAM, a un modelo que permitan al sector en estudio adoptar tecnología que agilice la burocracia y tramitología que obstaculiza el desarrollo del sector en estudio, éste modelo se le llama TAM4. Como primer estudio se aplica una encuesta a 68 de las 4560 empresas del sector, con un nivel de confianza y un error del 90% y ± 10% respectivamente, obteniendo que el 68% de estas estaría dispuesto a usar un modelo que posea los factores de: confianza y riesgo percibido. Como segundo estudio de investigación se diseña casos de estudio en cinco empresas agro exportadoras para validar el modelo TAM4, analizando el comportamiento a los largo de siete años de sus operaciones
comerciales, se determinó que el modelo TAM4 incrementa en 11% de las ventas de las empresas participantes en el periodo de estudio.",Universidad Nacional Mayor de San Marcos,Contribuciones al modelo de aceptación tecnológica aplicado al sector exportador de mercancías agrícolas,Doctor en Ingeniería de Sistemas e Informática
"La Serna Palomino, Nora Bertha","Lozada Yánez, Raúl Marcelo",2023,"Desarrolla el modelo de Realidad Aumentada denominado “Model for Augmented Reality
Applications with Gestural Interface for Children (MARAGIC)”. El modelo considera las
características cognitivas de niños en edad escolar, su validación y aplicación en el
diseño y desarrollo de dos Recursos Educativos Digitales (RED) que se emplean como
herramientas de apoyo en dos casos de estudio donde participan estudiantes que asisten
al 3° grado de Educación General Básica (EGB) en la ciudad de Riobamba de Ecuador. El trabajo pretende ser un aporte que oriente el adecuado diseño y desarrollo de este
tipo de herramientas tecnológicas educativas. Como método de estudio, se utiliza un
enfoque de investigación cualitativa en el que la recolección de datos se realiza
mediante la aplicación de una encuesta con elementos de referencia a la percepción
tanto a los niños participantes del estudio como a un grupo de docentes de nivel medio
y superior frente a la utilización de los recursos basados en MARAGIC. Se realiza
además un análisis cuantitativo que permite comparar el rendimiento académico de los
estudiantes antes y después de usar dichos recursos para los dos casos de estudio
ejecutados. Los resultados señalan que los docentes encuestados perciben al modelo
desarrollado como una oportunidad para mejorar los procesos de aprendizaje y que, la
utilización de los recursos educativos desarrollados siguiendo las orientaciones del
modelo MARAGIC mejora significativamente el rendimiento académico de los
estudiantes participantes del estudio en sus clases de matemática. La aplicación de este
tipo de recurso facilita los procesos de aprendizaje de los niños, mismos que se
muestran motivados y participan activamente en las actividades propuestas en los
recursos educativos diseñados a partir del modelo.",Universidad Nacional Mayor de San Marcos,Modelo de realidad aumentada que considere características cognitivas para aprendizaje de niños en edad escolar,Doctor en Ingeniería de Sistemas e Informática
"León Fernández, Cayo Víctor","Valencia Vivas, Gloria Maritza",2023,"Establece de qué manera las herramientas Web 2.0 influyen en las competencias genéricas Tuning en estudiantes de universidades públicas de Ecuador. En su desarrollo participaron 88 estudiantes universitarios de las carreras de Licenciatura en Ciencias Navales y Licenciatura en Ciencias Aeronáuticas de la Universidad de las Fuerzas Armadas del Ecuador, sus edades oscilaron entre de 20 a 21 años y con acceso a internet. El análisis de los datos revela que las
herramientas web 2.0 más utilizada por los estudiantes fue el Blog, seguido
por YouTube, representando el 68,2% del total de la muestra. Sin embargo,
determina que la Wiki y el Foro son las herramientas que tuvieron mayor
influencia en las competencias genéricas Tuning, mientras que el Blog, las
Redes Sociales y YouTube presentaron una menor influencia. Por lo cual, se concluye que el uso de herramientas web 2.0 tienen influencia positiva en las
competencias genéricas Tuning en estudiantes universitarios. Asimismo, se debe
incorporar estas herramientas en el proceso de enseñanza aprendizaje con el
uso adecuado y direccionado, en función de los objetivos y necesidades
específicas de cada contexto educativo.",Universidad Nacional Mayor de San Marcos,"Herramientas Web. 2.0 y su influencia en las competencias genéricas Tuning en estudiantes de universidades públicas de Ecuador, 2020: Caso UFA ESPE",Doctora en Ingeniería de Sistemas e Informática
"La Serna Palomino, Nora Bertha","Luna Encalada, Washington Gilberto",2023,"Desarrolla un ecosistema de nube social para enseñanza práctica de TI, mediante un modelo de implementación, que cumplan con los cuatro pilares educativos, basados en tres modelos de servicios de la computación en la nube, conocidos como Software como Servicio (SaaS), Plataforma como servicio (PaaS) e Infraestructura como Servicio (IaaS).",Universidad Nacional Mayor de San Marcos,Nube social para enseñanza práctica de tecnología de  información,Doctor en Ingeniería de  Sistemas e Informática
"Mauricio Sánchez, David Santos","Arcos Medina, Gloria de Lourdes",2023,"El propósito de esta investigación es contribuir en el aseguramiento de la calidad del software a través de la determinación de nuevos factores críticos de éxito y prácticas ágiles que influyen en las características de calidad de software. A a pesar de que existen estándares y modelos de calidad para asegurar la calidad en proyectos de desarrollo ágil, su complejidad en su aplicación es contraria a los principios propuestos por el agilismo, lo que impide tener un software con un adecuado nivel de calidad. Para cumplir con el objetivo propuesto se ha aplicado un diseño de investigación cualitativa, los resultados se obtuvieron mediante el análisis de residuos estandarizados a 146 cuestionarios dirigidos a personas involucradas en el proceso de
desarrollo de software. Esta investigación contiene una revisión sistemática de la literatura que permitió catalogar 148 factores críticos de éxito, 137 prácticas ágiles, 165 métricas y 71 atributos de calidad relacionados con el desarrollo ágil de software. Además, se presenta un modelo compuesto por cuatro componentes: 1) seis factores críticos de éxito obtenidas de las teorías del comportamiento humano y administrativo, 2) cuatro categorías de prácticas ágiles, 3) ocho características de calidad basadas en el estándar ISO/IEC 25010 , y 4) hipótesis que determinan la influencia entre los componentes del modelo propuesto. Las contribuciones realizadas como resultado de este estudio permitirán asegurar la calidad en el proceso de desarrollo de software ágil y tomar acciones para mitigar la influencia negativa de algunos
factores críticos y contribuir a la implementación de proyectos exitosos.",Universidad Nacional Mayor de San Marcos,Contribuciones para el aseguramiento de la calidad en el proceso de desarrollo de software ágil,Doctor en Ingeniería de Sistemas e Informática
"Papa Quiroz, Erik Alex","Cano Lengua, Miguel Angel",2023,"Desarrolla un algoritmo multiplicador proximal para resolver problemas de optimización convexa separable sobre conos simétricos y aplicarlo a problemas de clasificación binaria para máquinas de vectores de soporte. Este algoritmo tiene como fundamento teórico los espacios vectoriales de dimensión finita, además de un producto interior, un Álgebra de Jordan Euclidiana, distancias proximales, cono simétrico de segundo orden, entre otros conceptos teóricos. El algoritmo para clasificación de datos binario se desarrolló probando la buena definición, el análisis de la convergencia, el análisis de la complejidad de este. Este algoritmo surge como una nueva técnica el cual puede ser utilizado para la clasificación de datos, se probó su convergencia global bajo determinados supuestos, se realizó la implementó del algoritmo en el software Matlab versión 2017 (R2017a), en una computadora 8 th Gen Intel (R) Core (TM) i5-8250U CPU, 1.60 GHz 1.80 GHz, 4.00 GB, Windows 1064 bits. Posteriormente se realizó el diseño utilizando el marco metodológico Scrum. Para ello se desarrolló 3 sprint de acuerdo con los requerimientos de las historias de usuarios.
Se realizó un análisis comparativo con otros algoritmos proximales, así como también experimentos computacionales con el uso del algoritmo implementado.",Universidad Nacional Mayor de San Marcos,Un algoritmo multiplicador proximal para clasificación binaria en máquinas de vectores soporte,Doctor en Ingeniería de Sistemas e Informática
"Mauricio Sánchez, David Santos","Bravo Mullo, Silvia Jeaneth",2019,"Se analizaron seis aspectos sobre la detección de ataques DDoS: técnicas, variables, herramientas, ubicación de implementación, punto en el tiempo y precisión de detección. Este análisis permitió realizar una contribución útil al diseño de una estrategia adecuada para neutralizar estos ataques. En los últimos años, estos ataques se han dirigido hacia la capa de aplicación. Este fenómeno se debe principalmente a la gran cantidad de herramientas para la generación de este tipo de ataque. Por ello, además, en este trabajo se propone una alternativa de detección basada en el dinamismo del usuario web. Para esto, se evaluaron las
características del dinamismo del usuario extraídas de las funciones del mouse y del teclado. Finalmente, el presente trabajo propone un enfoque de detección de bajo costo que consta de dos pasos: primero, las características del usuario se extraen en tiempo real mientras se navega por la aplicación web; en segundo lugar, cada característica extraída es utilizada por un algoritmo de orden (O1) para diferenciar a un usuario real de un ataque DDoS. Los resultados de las pruebas con las herramientas de ataque LOIC, OWASP y GoldenEye muestran que el método propuesto tiene una eficacia de detección del 100% y que las características del dinamismo del usuario de la web permiten diferenciar entre un usuario real y un robot.",Universidad Nacional Mayor de San Marcos,Contribuciones para la Detección de Ataques Distribuidos de Denegación de Servicio (DDoS) en la Capa de Aplicación,Doctor en Ingeniería de Sistemas e Informática
"Mauricio Sánchez, David Santos","Oñate Andino, Mayra Alejandra",2022,"Determina la influencia de los 13 factores críticos de éxito propuestos en el principio de responsabilidad establecido en la norma ISO 38500. Una revisión exhaustiva de la literatura muestra que, en el contexto universitario no se han realizado estudios con estos propósitos. Este estudio empírico se realizó sobre el 76% de universidades ecuatorianas y muestra que los 13 factores analizados, 8 tomados de contextos
distintos al universitario y 5 extraídos desde las teorías del ámbito administrativo y comportamiento humano, tienen una influencia positiva en el éxito del Gobierno de las Tecnologías de la Información y el cumplimiento del principio de responsabilidad.",Universidad Nacional Mayor de San Marcos,Contribuciones al gobierno de las tecnologías de la información en el contexto universitario,Doctor en Ingeniería de Sistemas e Informática
"La Serna Palomino, Nora Bertha","Ayovi Ramirez, Marco Wellington",2022,"Pocas veces el éxito de una empresa surge de las casualidades. Por lo contario, se está más cerca del éxito y del bienestar cuando los resultados se producen por causalidades, lo que implica que se debe trabajar de manera planificada para encontrar los resultados deseados. Es también necesario manifestar que muchas
veces trabajando como debe ser, no se consiguen los resultados, debido a que hay una alta competitividad en las empresas, y por el hecho de que la gestión y planificación dentro de la empresa sean buenas, la competencia puede trabajar mucho mejor.
Las Pequeñas y Medianas Empresas (PYMES) representan más del 95% de las empresas en el mundo, radicando allí su importancia, ya que su aporte a la economía y a la empleabilidad mundial es significativo. Uno de esos segmentos de empresas que ocupan ese porcentaje, son las PYMES de Tecnologías de la Información y Comunicación (TIC), que por sus propias actividades y características son representativas en niveles de importancia para el crecimiento y desarrollo de la sociedad universal. Las PYMES de TIC probablemente tienen los mismos problemas que las PYMES de otros sectores, con dificultades organizativas y estructurales, necesarias para una adecuada gestión que propenda hacia el progreso, la competitividad, productividad y cada vez mayor rentabilidad. En los últimos años en diversos eventos y en publicaciones internacionales se ha tratado a profundidad los temas de Gestión de la calidad (GC), Gestión de la innovación (GI) y Gestión estratégica (GE), como pilares fundamentales para alcanzar la productividad (PR) de las organizaciones, y a partir de la PR obtener cada vez mayor Rentabilidad (RE).
Enfocado en que la GC, GI y GE integrada a las actividades de las PYMES de TIC pueden derivar en la Productividad de estas empresas, presentamos en esta investigación un Modelo Conceptual Corporativo, basado en GC, GI y GE para la Productividad de las PYMES de TIC. Estudio que se realizará en la geografía de la República del Ecuador.",Universidad Nacional Mayor de San Marcos,"Modelo corporativo basado en gestión de la calidad, gestión de la innovación y gestión estratégica, para pequeñas y medianas empresas del sector de las tecnologías de la información y comunicación",Doctor en Ingeniería de Sistemas e Informática
"Pró Concepción, Luzmila Elisa","Garcia Quilachamin, Washington Xavier",2021,"La revolución tecnológica del siglo XXI ha contribuido a la aplicación de algoritmos en
cámaras para la detección y reconocimiento de personas a partir de imágenes o videos
considerados aún complejos para una computadora. Por lo que las tecnologías asociadas a
la eficiencia energética y su uso en sistemas eléctricos surgen como problemas en la
aplicación de dispositivos inteligentes en: viviendas, edificios, empresas e instituciones
públicas y privadas. El objetivo de esta tesis es determinar de qué manera un algoritmo en
detección positiva de la imagen de una persona en tiempo real influye en la eficiencia
energética generada en un sistema de iluminación en una Smart Grid Home. Se realizó
una revisión sistemática de los algoritmos en detección de patrones, considerando los
criterios de Kitchenham y un estudio de campo, aplicado en instituciones públicas de la
provincia de Manabí - Ecuador, la cual fue validada mediante el análisis factorial de
confiabilidad relacionado con la prueba KMO, la esfericidad de Bartlett y el coeficiente
alfa de Cronbach. Se aplicaron técnicas en la detección de patrones y los resultados
obtenidos nos permiten considerar el modelo Support Vector Machines con un 92% de
reconocimiento y el algoritmo Viola-Jones con detección efectiva del 97,53%. De manera
experimental en relación con el algoritmo aplicado en un control de encendido y apagado
de un dispositivo de iluminación, los resultados determinaron que el 99.5% en la
detección de imágenes es positiva. Se concluye que la eficiencia y efectividad del
algoritmo aplicado en un control de encendido y apagado de un dispositivo de
iluminación mejora el consumo de energía eléctrica, siendo un aporte a la gestión
energética, seguridad y vigilancia de las personas y bienes materiales.",Universidad Nacional Mayor de San Marcos,Algoritmo en detección positiva de la imagen de una persona para la mejora en eficiencia energética en una Smart Grid Home,Doctor en Ingeniería de Sistemas e Informática
"Rodríguez Rafael, Glen Darío","Rosero Miranda, Raúl Hernán",2021,"Actualmente, en diferentes contextos, en mayor o menor grado, los
productos de software tienen acceso a bases de datos, siendo necesario
considerar las pruebas de regresión para productos de software, tales como
los sistemas de información. Esta tesis presenta dos aportes de investigación
que permiten motivar e introducir una nueva técnica/método para pruebas de
regresión por selección para software con acceso a datos.
El primer aporte establece los aspectos y factores considerados hasta
el año 2020, que evidencia la escasa aplicabilidad de las investigaciones de
la academia en la industria. El segundo aporte es el diseño de un método para
pruebas de regresión que realiza una combinación de técnicas de
clusterización probabilística no supervisada con valores de centroide
aleatorios y pruebas unitarias, que juntamente con el esquema de la base de
datos, determinar los casos de prueba relacionados con las modificaciones o
adiciones de nuevas funcionalidades del producto software. El método fue
validado empíricamente bajo el enfoque de ingeniería de software
experimental, mediante 32 de 37 pruebas de regresión de cinco proyectos de
software de pequeña y mediana escala, utilizando indicadores del campo de
la búsqueda y recuperación de información. Los resultados sugieren que el método propuesto mejora las pruebas de regresión en productos de software
con acceso a datos bajo entornos de desarrollo iterativo incremental.",Universidad Nacional Mayor de San Marcos,Método para reducir pruebas de regresión de software basado en acceso a los datos en entornos de desarrollo iterativo incremental,Doctor en Ingeniería de Sistemas e Informática
"Mauricio Sánchez, David Santos","Santisteban Pazos, José Luis",2021,"El emprendurismo es un responsable directo de impulsar el progreso
económico de las naciones, ya que al generar nuevas compañías se generan
nuevos puestos de trabajos y riquezas. Uno de los emprendimientos más
conocidos son las Startups de Tecnología de la Información (STI), este tipo
de empresas son entidades temporales e innovadoras que transforman
invenciones tecnológicas en productos innovadores. Conocedores de su
importancia que tienen este tipo de iniciativas empresariales, su tasa de
fracaso es alta en todo el mundo, solo el 20% logra superar los 3 primeros
años de operación. Por esta razón, se viene realizando diferentes estudios
sobre el éxito de la startup, identificando Factores Críticos de Éxito (FCE).
Sin embargo, aún hay factores que no han sido estudiados y que influyen de
manera positiva. Además, estos estudios no identifican cómo los factores
contribuyen en el éxito de las Etapas del Ciclo de Vida (ECV) de una STI.
Por lo tanto, esta tesis se identifican nuevos FCE en cada una de las ECV y
su éxito general, así mismo, se identifican y estandarizan las ECV de una
STI. Teorías como Capacidad de absorción, Confirmación de la expectativa,
Modelo de éxito de un sistema de información, y la Capacidad dinámica
ofrecen FCE no investigados en materia de emprendimiento tecnológico
(Capacidad de absorción de conocimiento, Alto rendimiento percibido del
producto, Calidad del producto, Satisfacción del cliente, y Capacidad
dinámica). Se llevó a cabo dos estudios con pruebas estadísticas para
demostrar, que los nuevos FCE tienen una contribución “Alta” y “Muy alta”
sobre el éxito de una STI, con un 95% de certidumbre en la prueba de
hipótesis T-Student.",Universidad Nacional Mayor de San Marcos,Contribuciones al desarrollo de una Startup de TI: ciclo de vida y factores críticos de éxito,Doctor en Ingeniería de Sistemas e Informática
"La Serna Palomino, Nora Bertha","Cadena Moreano, José Augusto",2021,"La presente investigación se desarrolla en el marco de
los sistemas de reconocimiento facial automático de imágenes, que consisten en
procesar las imágenes de caras de personas utilizando métodos estadísticos y
matemáticos de extracción de características y de clasificación de imágenes, para
conocer si un individuo se encuentra en una determinada clase, y finalmente hallar su
identidad. El tratamiento automático de una cara es complicado, debido a que se
presenta varios factores que le afectan, como la posición de la cara, la expresión, la
edad, la raza, el tipo de iluminación, el ruido, y objetos como lentes, sombrero, barba
entre otros. El procesamiento se realiza de forma global, en donde se procesa toda la
cara. Se sabe que procesar las imágenes de manera global es más rápido, práctico y
fiable que las basadas en rasgos. Además, se conoce que procesar imágenes en tres
dimensiones es más real y consistente que en dos dimensiones. El principal objetivo
de la tesis que se propuso fue desarrollar una técnica eficiente de reconocimiento facial
con rasgos globales, y con imágenes en tres dimensiones. Para ello, se seleccionó los
algoritmos más eficientes para extracción de características, filtros de Gabor, y el
algoritmo para clasificación, máquina de vectores de soporte (SVM). Este último
algoritmo, su eficiencia varía de acuerdo a la función núcleo o kernel, por ello en esta
tesis se trabajaron con tres kernel: líneal, gauseano y cúbico. Estos sistemas constan
de dos procesos necesarios: 1) Entrenamiento, y 2) Pruebas. Lo que permitió establecer
un modelo de reconocimiento facial global para dos y tres dimensiones
respectivamente. La técnica fue procesada primero para imágenes 2D, luego para
imágenes 3D. Y se utilizó el método de validación cruzada en ambos casos para
aprobarlo. Los mejores resultados obtenidos con la técnica alcanzada son 96% de
eficiencia con base de datos de imágenes de dos dimensiones; y 98,4% con base de
datos de imágenes de tres dimensiones. Finalmente, se hace una comparación de los
resultados alcanzados con otros trabajos de investigación similares, obteniéndose
mayor eficiencia con este trabajo.",Universidad Nacional Mayor de San Marcos,Técnica eficiente para reconocimiento facial global utilizando wavelets y máquinas de vectores de soporte en imágenes 3D,Doctor en Ingeniería de Sistemas e Informática
"Aguilar Alonso, Igor Jovino","Mera Macias, Angel Cristian",2021,"Para desarrollar una buena gestión de servicios tecnológicos es necesario
contar con un catálogo de servicios de tecnologías de la información; sin
embargo, según la literatura se evidencian bajos niveles de implementación
de este catálogo en entidades públicas, esto se corroboró mediante un estudio
de campo realizado en 30 instituciones públicas de la provincia de Manabí,
República del Ecuador, evidenciando que de las pocas entidades que
contaban con un catálogo de servicios (22% parcialmente y 5% totalmente
implementado), la mayoría realizan sus operaciones de manera empírica; su
falta de implementación se da por factores como la no obligatoriedad para
desarrollarlo, el desconocimiento del tema y de las normas y/o prácticas
existentes. Ante esta situación se propuso como objetivo general desarrollar
una metodología para la construcción del ITSC basada en mejores prácticas
de ITSM para contribuir a la gestión del catálogo de servicios de TI en
entidades públicas mediante la automatización de sus actividades. Para
lograrlo, se realizó una revisión sistemática de la literatura, donde se
determinaron 35 estudios que contribuyeron al desarrollo de la propuesta, la
misma que fue construida siguiendo el paradigma de la investigación en las
ciencias del diseño; además, la propuesta fue probada mediante casos de
estudio en tres entidades públicas (dos de Ecuador y una de Perú) aplicando
ocho factores de calidad para la evaluación de artefactos en la construcción
de catálogos, y comprobando los niveles de exactitud (80.81%), sensibilidad
(87.59%), precisión (90.41%) y Puntuación F1 (88.93%), derivados de la
matriz de confusión; finalmente, se evaluó la contribución de la propuesta por
parte de 46 profesionales de tecnologías de la información que laboran en
entidades públicas ecuatorianas y que conocen del catálogo, corroborando
que la propuesta contribuye a la identificación y clasificación de servicios, la
retroalimentación del catálogo y a la automatización de la gestión de servicios
de tecnologías de la información, y por ende a la automatización de la gestión
del catálogo de servicios de tecnologías de la información.",Universidad Nacional Mayor de San Marcos,Metodología para la construcción del catálogo de servicios de TI basada en mejores prácticas de ITSM en entidades públicas,Doctor en Ingeniería de Sistemas e Informática
"Gamboa Cruzado, Javier Arturo","Chancusig Chisag, Juan Carlos",2021,"La investigación se encuentra enmarcada en la implementación de un modelo de
adopción de las TIC para el proceso enseñanza – aprendizaje aplicadas a la Universidad
Técnica de Cotopaxi, específicamente para mejorar la educación universitaria, esto se
debe a los constantes avances en áreas de multimedia digital y la tecnología didáctica,
por lo que los organismos se comprometen a mejorar su organización académica y sus
procesos educativos. A través del uso de tecnologías de colaboración, las instituciones educativas tratan que la administración del conocimiento se integre interiormente y externamente de la propia
universidad, ya que se consideran que ello ayuda a la innovación educativa. En este
contexto y viendo la problemática de estudio, la solución es la aplicación del nuevo
proceso de aceptación de las TIC mejorará el proceso enseñanza – aprendizaje en los
estudiantes universitarios.
En los últimos años se ha desarrollado el uso de TIC en los distintos niveles de
educación y específicamente en las universidades actualmente cuentan con nuevos
recursos tecnológicos muy avanzados, en el caso concreto de la universidad donde se
está desarrollando la investigación el objetivo de estudio es, desarrollar un nuevo
modelo de adopción de TIC, con lo cual es posible dar solución al proceso educativo.
Se aplica una metodología de estudio basado en un abordaje sistemático de la literatura
de los teórías de las TIC tomando como referencia el primer modelo que apareció el
Modelo TAM que fue desarrollado por Davis en 1986 en las universidades del mundo.
La investigación arrojó los resultados esperados con el desarrollo del nuevo modelo de
adopción de las TIC, con la utilización de herramientas colaborativas de uso en un
Ambiente de Aprendizaje Colaborativo (CLE) para la elaboración de los constructos,
alfa de cronbach, varianza, colinealidad, correlaciones y la utilización de software
estadístico como el minitab se validó las hipótesis planteadas.",Universidad Nacional Mayor de San Marcos,"Implementación de un modelo de adopción de la tecnología de información y comunicación para el proceso de enseñanza–aprendizaje en la Universidad
Técnica de Cotopaxi",Doctor en Ingeniería de Sistemas e Informática
"Bayona Oré, Luz Sussy","Morales Lozada, José Vicente",2020,"Desarrolla un modelo conceptual integrado de influencia de factores en los niveles de desarrollo de e-gobierno, y que, mediante la determinación de los factores críticos permita mejorar la presencia y prestación de e-servicios en los municipios. El tipo de investigación aplicado en el presente trabajo es relacional, descriptiva y con un punto de vista cuantitativo. El diseño del estudio es no experimental, en virtud de que se analizan datos, logrados mediante la encuesta. Con aplicación del análisis multivariante y la técnica de componentes principales se determinó que todos los factores propuestos poseen las características de esfericidad necesarias para indicar que las variables están relacionadas entre sí. Además, la prueba de KMO con un 95% de confianza, permitió verificar que 27 de un total de 55 relaciones fueron válidas. Los resultados posibilitaron determinar qué factores son los más influyentes en cada nivel de desarrollo de e-gobierno, lo cual permitió validar el modelo integrado final de influencia de factores. Este modelo fue implementado en 5 municipios aplicando el método de estudio de casos, cuyos resultados muestran que, al contar con un modelo integrado se logra mejorar el índice de desarrollo de e-gobierno, parámetro útil para la toma acertada de decisiones de la alta dirección. Se concluye que al integrar modelos de e-gobierno y factores críticos, se logra mejorar la prestación de e-servicios mediante la implementación de los mismos, logrando mejorar el desarrollo de e-gobierno en forma global y por etapas de desarrollo.",Universidad Nacional Mayor de San Marcos,Modelo conceptual integrado de e-gobierno para mejorar la prestación de e-servicios en las municipalidades de la provincia de Tungurahua - Ecuador,Doctor en Ingeniería de Sistemas e Informática
"Delgadillo Ávila, Rosa Sumactika","Avila Pesántez, Diego Fernando",2020,"Los Serious Games son juegos digitales con propósito educativo que utilizan principios pedagógicos e incorporan elementos de motivación y entretenimiento. La complejidad del desarrollo de los Serious Games ha establecido varios enfoques, que involucran a las metodologías de Ingeniería de Software, con el diseño para juegos comerciales a través del Documento del diseño del juego, y el entorno del aprendizaje basado en el Diseño Instruccional. Cada uno de ellos fue analizado a través de una revisión sistemática de la literatura, que permitió establecer las principales fases, elementos pedagógicos y factores, con el objetivo de generar un modelo conceptual integrador para el diseño de los SG, utilizando tecnologías emergentes y una interfaz de usuario natural. Este modelo se estructuró en cuatro fases: Análisis, Diseño, Desarrollo y Evaluación, que identifica y valida los roles de todos los componentes, para lograr los objetivos educativos deseados, integrando el aprendizaje en una sinergia con los episodios de juegos, que permitió crear un Serious Game que refuerza la estimulación cognitiva en los niños. Para la validación del modelo propuesto se implementó el juego denominado ""ATHYNOS"" que ayuda en las terapias a los niños con Trastorno de Déficit de Atención e Hiperactividad, dispraxia y discalculia. A través de un estudio de caso con tres unidades de análisis se reportó sus beneficios, mejorando la motivación de los participantes, proporcionando un soporte a los procesos de aprendizaje. Además, se fortaleció el nivel de concentración, coordinación ojo-mano, habilidades motoras y de refuerzo cognitivo en operaciones aritméticas, con experiencias de aprendizaje inmersivas, con una mayor participación y colaboración en las diferentes actividades dentro del juego, que se vieron reflejadas en el desempeño académico escolar.",Universidad Nacional Mayor de San Marcos,Modelo integrador para el diseño de Serious Games con realidad aumentada,Doctor en Ingeniería de Sistemas e Informática
"Mauricio Sánchez, David Santos","Albán Taipe, Mayra Susana",2019,"Identifica una limitada producción científica que analiza factores de deserción desde la perspectiva del estudiante, que es el actor principal de la deserción, y la construcción de modelos híbridos de predicción que permitan comprender mejor manera el problema de la deserción en las universidades. El objetivo consiste en contribuir al proceso de predicción de la deserción estudiantil universitaria a través del estudio integral de factores, técnicas y herramientas de minería de datos usados con este fin. Se concluye que la predicción de la deserción en las universidades puede variar, ya que dependerá de los factores de ingreso, del contexto educativo estudiado, del entorno de educación aplicado, y de los antecedentes de los estudios para los que fueron usados. Por otro lado, se considera importante determinar si es suficiente con predecir la deserción o si se requiere incorporar estudios que establezcan estrategias para mitigar la deserción en las instituciones de educación superior.",Universidad Nacional Mayor de San Marcos,Contribuciones a la predicción de la deserción universitaria a través de minería de datos,Doctor en Ingeniería de Sistemas e Informática
"Mauricio Sánchez, David Santos","Castañeda Vargas, Pedro Segundo",2019,"La productividad en las fábricas de software es dado por el esfuerzo realizado para la producción del software, siendo muy importante porque permite que las organizaciones logren una mayor eficiencia y eficacia en sus actividades. Uno de los pilares de la competitividad es la productividad, la cual está relacionada al esfuerzo requerido para cumplir con las tareas asignadas, sin embargo, no existe una forma estándar de medirla. En este trabajo, se presenta un modelo basado en Análisis Envoltorio de Datos (DEA, por las siglas del inglés Data Envelopment Analysis) para evaluar la eficiencia relativa de las fábricas de software y sus proyectos, a fin de medir la productividad en la Componente de Producción de Software de la Fábrica de Software a través de las actividades que se realizan en sus diferentes unidades de trabajo. El modelo propuesto consta de dos fases, en la cual se evalúa, respectivamente, la productividad de la fábrica de software y la productividad de los proyectos que esta realiza. Pruebas numéricas sobre 6 fábricas de software con 160 proyectos implementados en el Perú muestran que el modelo propuesto permite determinar las fábricas de software y los proyectos más eficientes.",Universidad Nacional Mayor de San Marcos,Modelo de medición de la productividad para fábricas de software,Doctor en Ingeniería de Sistemas e Informática
"Mauricio Sánchez, David Santos","Wong Portillo, Lenis Rossi",2019,"En los últimos años han surgido diferentes problemas en la elicitación de requisitos de software, lo cual ocasiona que se obtenga requisitos deficientes. Por ello, la elicitación es una pieza clave para la industria del software, puesto que los requisitos con mala calidad son una de las causas del fracaso de los proyectos de software. Por esta razón, se han realizado diferentes estudios sobre la elicitación de requisitos, sin embargo, en la literatura se han identificado factores que afectan algunas actividades del proceso de elicitación, de ahí que se hallen actividades que no han sido estudiadas, pero que son importantes en el proceso, puesto que obtener un “buen requisito” depende de todas las actividades del proceso en su conjunto. Además, estos estudios no analizan cómo las actividades del proceso de elicitación contribuyen en la calidad del requisito. En la presente tesis se introduce nuevos factores que influyen en cada una de las actividades del proceso de elicitación de requisitos, así mismo, se identifican las cualidades que estas actividades deben cumplir con el fin de garantizar un “buen requisito”. Las teorías del Comportamiento Organizacional, Aprendizaje Organizacional, Argumentación, entre otras, proporcionan factores no estudiados en el área de elicitación de requisitos (capacidad de aprendizaje, capacidad de negociación, personal estable, confianza, estrés y semi-autonómica). Dos estudios empíricos demuestran, a través de pruebas estadísticas, que los factores mencionados tienen influencia entre “alta” y “muy alta” sobre las actividades del proceso de elicitación, además, que estas actividades deben cumplir dieciséis cualidades para obtener un buen requisito. Finalmente, ambos estudios se corroboran con la prueba de hipótesis T-Student, con el 95% de confianza.",Universidad Nacional Mayor de San Marcos,"Contribuciones en el proceso de elicitación de requisitos: factores, actividades y cualidades",Doctor en Ingeniería de Sistemas e Informática
