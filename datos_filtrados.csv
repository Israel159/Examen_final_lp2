dc.contributor.advisor,dc.contributor.author,dc.date.issued,dc.description.abstract[es_PE],dc.publisher[es_PE],dc.source[es_PE],thesis.degree.name[es_PE]
"Salinas Flores, Jesús Walter||Rosas Villena, Fernando René","Rado Huaringa, Joao Manuel",2014,"El objetivo de la investigación fue probar la hipótesis que la tasa de error de clasificación utilizando  el análisis discriminante con algoritmos genéticos es menor a la que se obtiene con el análisis discriminante lineal de Fisher. La aplicación se efectuó en la predicción del rendimiento en el examen de admisión de la Universidad Nacional Agraria La Molina de los postulantes cuya preparación se realizó en su Centro de Estudios Preuniversitarios. En la técnica de algoritmos genéticos  se empleó el método de selección, cruce y mutación que permitió realizar la búsqueda de funciones discriminantes con error mínimo. Los resultados del estudio indican que el análisis discriminante con algoritmos genéticos proporcionó una función discriminante más eficiente que la proporcionada por Fisher.",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
"Porras Cerrón, Jaime Carlos","Rado Huaringa, Joao Manuel",2019,"En esta investigación se realizó la aplicación de los métodos de equiparación lineal y equipercentil a los puntajes obtenidos de los postulantes a los exámenes de admisión 2016-I y 2016-II de la Universidad Nacional Agraria La Molina. El desarrollo se realizó en las seis áreas que se evalúan en el examen de admisión: Razonamiento Verbal, Razonamiento Matemático, Matemática, Física, Química y Biología. El indicador utilizado para comparar ambos métodos fue el error estándar de equiparación. Entre los resultados más importantes se encontró que el método de equiparación lineal tuvo un mejor ajuste que el método equipercentil. Respecto a la dificultad de los exámenes de admisión, se obtuvo que el examen 2016-II presentó una mayor dificultad que el examen 2016-I. Finalmente, en relación a las seis áreas evaluadas en los exámenes, fue Matemática la que presentó una mayor dificultad en el examen de admisión 2016-II que en el 2016-I.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Rosas Villena, Fernando René","Vivanco Huaytara, Fredy",2018,"El objetivo general de la investigación es la identificación entre las técnicas del Análisis Discriminante Lineal de Fisher y Máquina de Soporte Vectorial la que presenta mejores indicadores de clasificación del rendimiento de los postulantes en la prueba de admisión 2015-II en la Universidad Nacional Agraria La Molina (UNALM). Ambas técnicas estadísticas se aplicaron en dos oportunidades, en la primera se evaluó el resultado en la prueba admisión de la UNALM de los postulantes que no se prepararon en el CEP-UNALM y que ingresaron o no a la universidad (aplicación 1) y en la segunda se evaluó resultado en la prueba admisión de la UNALM de los postulantes que si se prepararon en el CEP-UNALM y que ingresaron o no a la universidad (aplicación 2). Los resultados muestran que la técnica Máquina de Soporte Vectorial presenta mejores indicadores de clasificación que el Análisis Discriminante Lineal de Fisher.",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
"Rosas Villena, Fernando René","Salazar Vega, Rolando Jesús",2019,"El propósito principal de la investigación fue comprobar si el rendimiento académico de los estudiantes en el curso de Estadística General de la Universidad Nacional Agraria La Molina (UNALM) es explicado a través de un modelo propuesto de Ecuación Estructural de tres factores. El primero denominado “desempeño docente”, medido por las variables: planificación del curso, dominio del curso, métodos y recursos de instrucción, obligaciones docentes, método evaluativo, y motivación e interacción con los alumnos; el segundo llamado “autoconcepto”, medido por las variables: académico/laboral, social, emocional, familiar y físico y finalmente el tercero “rendimiento pasado”, medido a través del promedio ponderado acumulado. Los datos utilizados corresponden a las notas de los alumnos matriculados en el ciclo académico 2014-I en el curso de Estadística General, al promedio ponderado acumulado; y los valores se registraron en la escala de Likert de 1 al 10 de las encuestas de desempeño docente y autoconcepto. Estos dos instrumentos, cumplen con los requisitos de confiabilidad y validez al registrar en ambos casos indicadores por encima de los mínimos aceptables. El modelo de ecuación estructural propuesto fue reespecificado (mejorado) mediante la inclusión de una nueva relación de interdependencia, el rendimiento pasado como predictor del autoconcepto. Se verificó el ajuste del modelo de ecuación estructural reespecificado a través de los principales indicadores de ajuste absoluto e incremental. Entre los resultados más importantes de la investigación se verificó que el factor rendimiento pasado es el mejor predictor del factor rendimiento académico de los estudiantes en el curso de Estadística General y que los factores desempeño docente y rendimiento pasado explican al factor autoconcepto.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Porras Cerrón, Jaime Carlos","Sucari Sucari, Ruben Elvis",2018,"En la presente tesis se desarrolló el método de clasificación llamado Análisis Discriminante No Métrico, y se comparó su desempeño con el Árbol de Clasificación CHAID y la Regresión Logística Multinomial, los cuales también son métodos que no necesitan la condición de normalidad multivariada, linealidad ni varianza homogénea para las variables independientes. Esta comparación de desempeño fue evaluado mediante la Validación Cruzada. Para la realización del estudio comparativo de estos clasificadores se utilizó conjuntos de datos que son proporcionados por la Universidad de California Irving (UCI). Se concluye que la Regresión Logística Multinomial tiene mejor desempeño en la clasificación de datos teniendo en cuenta la tasa de clasificación promedio y el tiempo de procesamiento",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Magister Scientiae - Estadística Aplicada
,"Porras Huamán, Beatriz Eta",2014,"El turismo en el Perú ha venido creciendo rápidamente en los últimos años MINCETUR - Plan Estratégico Nacional de Turismo 2012 - 2021 – PENTUR, pág. 11). En el proceso de la investigación turística se recopila los datos, para ser analizados y efectuar una crítica, con fines de alcanzar elementos de trabajo hacia otras fases de desarrollo turístico, las mismas que permitirán obtener nuevos conocimientos que como aporte se utilizarán en la actividad turismo.En este sentido el presente trabajo busca caracterizar o establecer cuáles son los perfiles de los turistas internos en el Perú de los niveles socioeconómicos medio y alto con la finalidad de diversificar la estrategia turística y de este modo poder administrar en forma sostenida recursos del sector público como el privado. Esta investigación utiliza una muestra de 2,400 turistas de los principales destinos turísticos del Perú. El objetivo planteado para el desarrollo del presente trabajo es la determinación de los grupos de individuos observando sus características socio-demográficas, los hábitos de vida y las preferencias con respecto a viajes al interior del país. La metodología empleada corresponde a la técnica de agrupamiento Análisis de Conglomerados Bietápico. La pregunta de investigación planteada para el desarrollo del proyecto se orienta a determinar los diversos tipos de perfiles de turistas nacionales en los niveles socioeconómicos A, B y C. Finalmente se determinó la homogeneidad de los grupos en función a la variabilidad intra-grupos, encontrándose dos grupos, descritos estos en función a los estadísticos encontrados dentro de cada grupo.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
,"Ramírez Soplin, Magally Loidit",2014,"El presente estudio de investigación se centró en identificar los perfiles más adecuados, en una muestra de 8, 504 clientes que realizaron transacciones crediticias en el primer trimestre del año. Se agruparon los casos mediante las técnicas de segmentación: K-means, Bietápico y Kohonen, utilizando variables cuantitativas y categóricas. De las tres técnicas, la que obtuvo mayor medida de silueta de cohesión y separación, fue K-means, indicando una estructura “buena” en cuanto a la cohesión al interior de los grupos y la separación de los mismos. Por otro lado, también se analizó las proporciones de los conglomerados, siendo la técnica K-means la que presentó las proporciones más adecuadas en función a las variables de historial crediticio y transacciones realizadas. Posterior a la obtención de los conglomerados, se procedió al proceso de obtención de la reglas de clasificación, mediante la técnica de regresión logística multinomial, la cual nos permitirá realizar predicciones futuras. El procedimiento se aplicó a la muestra particionada, es decir, una parte de entrenamiento y otra de comprobación. Finalmente, se obtuvo una adecuada tasa de eficiencia en ambas muestras. Además, los análisis permitieron identificar a dos conglomerados que muestran una alerta para la empresa, es decir necesitan ser gestionados de forma oportuna, ya que constituyen un futuro comportamiento de no pago de acuerdo a la caracterización obtenida de dichos conglomerados.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"Espinoza Villanueva, Luis Enrique","Linares Torres, Miguel Angel",2018,"Se realizó una investigación cuantitativa, con el fin de identificar los factores más influyentes al momento de evaluar el servicio que ofrece el centro educativo, el instrumento de medición es la encuesta vía telefónica y se  entrevistó a los  padres de familia que matricularon a sus hijos en el año 2016. La recolección de información se realizó por medio de un cuestionario,  seguidamente se analizó en una base de datos y con procedimientos estadísticos se ejecutó pruebas para determinar  qué factores  son los más influyentes al momento de evaluar la calidad del servicio. Asimismo, se analizó el entorno como la infraestructura, calidad de servicio, equipamiento tecnológico y calidad educativa.  Finalmente con los resultados de las pruebas se procedió a elaborar estrategias de mercado para un óptimo posicionamiento",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
,"Vicente Vasquez, Juana Mercedes",2014,"El estudio tiene como objetivo clasificar a las familias encuestadas en los distritos de San Pablo, San Luis y San Bernardino de la provincia de San Pablo en el departamento de Cajamarca según un conjunto de variables socio-económicas. Estos datos corresponden a una investigación realizada por un grupo de personas que laboran en la Universidad del Pacifico, la encuesta fue realizada en Diciembre del 2006. Se desea clasificar a las familias para poder brindar un mejor control en el estudio longitudinal de los proyectos a ser evaluados. Para esto, al culminar la encuesta se planteó una clasificación preliminarmente la existencia de 4 grupos de familias. Para verificar esta clasificación se utilizó el “Análisis de Clúster”, que es un método multivariado de clasificación. Para el procesamiento de los datos se utilizó el programa “Minitab versión 17” y “Microsoft Excel”",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
,"Huamaní Miranda, María Alejandra",2014,"La realidad competitiva que en estos días enfrentan las entidades bancarias ha provocado que éstas no sólo concentren sus esfuerzos de marketing exclusivamente en estrategias de captación de clientes, sino también en estrategias de retención y ﬁdelización; la fuga de clientes es una situación que afecta la rentabilidad de la gran mayoría de las instituciones bancarias dado que se invierte mucho más en la captación de clientes que en campañas para la retención, por ello, es un tema de intensivo estudio cientíﬁco en los últimos años. Las entidades bancarias requieren contar con herramientas que les permitan estimar probabilidades de fuga para su cartera de clientes y así decidir sobre que clientes concentrar sus esfuerzos de retención. En el presente trabajo se utilizó la regresión logística de respuesta binaria  y el algoritmo de árbol de clasificación CART para predecir y clasificar a los clientes con riesgo de fuga y así identificar el mejor modelo explicativo de retención de clientes con riesgo de fuga para una entidad bancaria. El modelo que mejor explica el riesgo de fuga de un cliente fue la Regresión Logística binaria que obtuvo como variables predictoras número de transacciones, ingreso bruto, número de tarjetas usadas y línea de crédito. Las variables identificadas permitirán a la entidad bancaria reorientar las estrategias en las campañas de retención de clientes.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"Chue Gallardo, Jorge","Fernández Vásquez, Richard Fernando",2018,"En la actualidad las entidades bancarias conviven con clientes que no cumplen con sus obligaciones crediticias y se exceden del plazo estipulado acordado con el banco, a estos clientes se les denomina clientes morosos, por tal motivo el objetivo del presente trabajo es determinar el modelo de regresión binaria bayesiano con enlace asimétrico más adecuado para clasificar a los clientes que incumplirán sus pagos de sus tarjetas de crédito según sus probabilidades de mora en la entidad bancaria UNIBANK y haciendo uso de las variables más significativas. Se realizó un análisis comparativo entre los modelos de regresión bayesiana con enlaces asimétricos cloglog, power logit y scobit, y se determinó que el modelo de regresión binaria bayesiano con enlace asimétrico cloglog fue el más adecuado para clasificar a los clientes que incumplen sus obligaciones crediticias con sus tarjetas de crédito en la entidad bancaria UNIBANK según su probabilidad de mora, pues este modelo presentó un valor mucho mayor de sensibilidad que los modelos power logit y scobit, siendo las diferencias 8.5% y 9.1%, respectivamente",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Magister Scientiae - Estadística Aplicada
"Miranda Villagomez, Clodomiro Fernando","Rubio Donet, Jorge Luis",2018,"La presencia de observaciones atípicas en un conjunto de datos es una de las causas que generan distorsiones en el análisis. La detección de dichas observaciones puede ayudar a una correcta evaluación de las tendencias en el comportamiento de los datos. Para el caso de datos multivariados se han desarrollado diversos métodos que permiten la detección de comportamientos atípicos, basados en métodos gráficos, y otros asumiendo una distribución normal multivariada. No obstante, en muchos casos el supuesto de normalidad multivariada no se cumple. El presente trabajo propone una prueba no paramétrica basada en la aplicación del método Bootstrap, utilizando como indicador de similitud a las distancias entre las representaciones obtenidas con series finitas de Fourier, propuesta por Andrews. El método propuesto permite detectar datos multivariados atípicos, combinando la significación estadística de la prueba Bootstrap y el análisis gráfico sugerido por Andrews, y que puede ser también aplicado a datos medidos en una escala ordinal. El método fue aplicado a cuatro conjuntos de datos, encontrando resultados satisfactorios en todos los casos.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
,"Neira Campos, Mike Alex",2017,"Este trabajo monográfico gira en torno a las series de tiempo con Redes Neuronales Artificiales a fin de realizar pronósticos. Para este propósito, el presente trabajo se compone de 4 capítulos, donde el primer capítulo versa sobre las definiciones y conceptos principales del pronóstico de una serie temporal que otorga validez teórica a la investigación. En el segundo capítulo, el lector podrá encontrar la descripción de las Redes Neuronales Artificiales en la predicción de datos. El tercer capítulo, da cuenta de las implicaciones de una metodología del pronóstico de datos, utilizando las Redes Neuronales Artificiales. Finalmente, el capitulo 4, dilucida la aplicabilidad de las mencionadas Redes y se hace un paralelo con otros métodos de pronóstico con el objeto de resaltar sus diferencias y características. En conclusión, podemos decir que el lector podrá encontrar en este trabajo las etapas necesarias para llevar a cabo la elaboración de una red neuronal que pueda predecir valores futuros de una serie de tiempo.",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
,"Flores Espinoza, María del Carmen",2014,"El presente trabajo tiene como propósito esencial, realizar una segmentación psicográfica, en base a valores y estilos de vida, así como las actitudes frente a la tecnología de los habitantes de Lima Metropolitana y Callao de los niveles socioeconómicos A, B, C y D, entre 15 y 69 años de edad. Se utilizó una muestra de 906 casos, a los cuales se le aplicó un cuestionario estructurado que contenía 35 atributos previamente definidos en base a una investigación cualitativa, y otras preguntas de control, tal como edad, nivel socioeconómico y tenencia de productos tecnológicos en el hogar. Del total de 35 atributos evaluados los cuales se redujeron en 8 factores tras realizar un análisis factorial. Seguidamente se realizó un análisis clúster, donde se identificaron 5 segmentos caracterizados por los atributos previamente mencionados: Aquellos que se encuentran Orientados a la tecnología; los que, a pesar de tener productos tecnológicos prefieren estar esconectados; los que se encuentran Orientados al poder, los que tienden a ser Tradicionales por sus actitudes frente a las costumbres, y aquellos Orientados a los valores que tienen como eje virtudes tales como la solidaridad, sencillez, etc.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
,"Acosta Pizarro, Diana Rosa",2014,"El presente estudio tiene como objetivo principal identificar el perfil de los clientes del departamento de Lima que aceptan una tarjeta de crédito de una entidad financiera cuando el productoes ofrecido por el canal de ventas Call Center. Se utilizó la técnica de Árboles de Clasificación CHAID Exhaustivo el cual proporciona buenos resultados de clasificación correcta de los clientes que aceptan una tarjeta de crédito vía Call Center. Se consideró una muestra en un período de cinco meses (Diciembre 2013 a Abril 2014) logrando identificar que las variables más significativas que aportan en el modelo son la edad, el ingreso neto mensual y el tipo de tarjeta que se le ofrece al cliente. Estas variables presentan importancia relevante en el cliente para tomar la decisión de aceptar una tarjeta de crédito. Los resultados obtenidos mediante el algoritmo CHAID Exhaustivopermitieron identificar los patrones que definen el perfil de los clientes que aceptan una tarjeta de crédito vía Call center con el fin de ser más efectivos, aumentando el número de ventas, reduciendo el número de llamadas, minimizando costos y tiempo",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"López de Castilla Vásquez, Carlos","Mendoza  Quevedo, Diego Alonso",2013,"La presente investigación tiene por objetivo principal determinar si las distribuciones asimétricas skew-normal y skew-tson buenos modelos para describir datos relativos al coste de los siniestros. Para lo cual se analizaron dos conjuntos de datos de una compañía de seguros, ajustando las distribuciones en estudio y las tradicionales a estos datos. Luego se compararon las distribuciones empleando el Criterio de Información de Akaike (AIC) y del Logaritmo de la función de Verosimilitud, complementados con la prueba de bondad de ajuste Kolmogorov-Smirnov,obteniendo resultados positivos. Además, se calcularon medidas de riesgo como el Valor en Riesgo (VaR) y el Valor en Riesgo Condicional (TVaR), que brindaron mayor solidez a los resultados previos: los costes de los siniestros en los seguros se ajustan bien a las distribuciones asimétricas skew-normal y skew-t.",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
"Soto Rodríguez, Iván Dennys","Cormán Trujillo, Juan",2020,"La entidad pública brinda servicios de capacitación para el Sector Vivienda, Construcción y Saneamiento, inicia su vida institucional en el año 1977. Tiene como finalidad la formación de los trabajadores del sector construcción, la educación superior no universitaria, el desarrollo de investigaciones vinculadas a la problemática de la vivienda y edificación, así como la propuesta de normas técnicas de aplicación nacional. Y se ha establecido en doce sedes a nivel nacional y capacita a más de 30,000 trabajadores en el desarrollo de nuevas tecnologías. La entidad según su estructura organizacional se encuentra conformado por: Consejo Directivo, Gerencia General, Oficina de Administración y Finanzas, Oficina de Planificación y Presupuesto, Gerencia de Formación profesional y Gerencia de Investigación y Normalización; cabe señalar que el Departamento de Informática forma parte de la Oficina de Administración y Finanzas, en la cual se desempeñó el cargo.",Universidad Nacional Agraria La Molina,,Ingeniero Estadístico e Informático
,"Montaño Miranda, Beatriz del Carmen Lidia",2013,"El presente estudio tuvo como objetivo principal verificar la disponibilidad de productos en el almacén de un Supermercado, así las decisiones a tomar ante la falta de productos serían más certeras y se tendría un mejor panorama al respecto de la situación del abastecimiento del Supermercado. El trabajo consiste en un modelo de predicción de quiebres de stock para un supermercado. Se analizaron las ventas en unidades, el stock de los productos, los despachos de proveedores, los días del mes de Agosto (con ventas y sin ellas) de un grupo de productos de diferentes gerencias del supermercado (Abarrotes Comestibles, Abarrotes no Comestibles y Bebidas). Con la información recopilada se consideró el valor de la variable dependiente (en quiebre con valor 1 y O en caso contrario). La selección de variables por Boruta permitió obtener un modelo con menos cantidad de variables y con un mejor ajuste al realizar el análisis usando la regresión logística en comparación con la selección de variables por Stepwise.",Universidad Nacional Agraria La Molina,,Ingeniero Estadístico e Informático
"Rino Sotomayor, Ruíz","Hurtado Oliva, Katherine Vanessa",2015,"El presente trabajo tiene como objetivo estimar el modelo Vector Autorregresivo (VAR) que permita describir simultáneamente el comportamiento de la morosidad de cartera, el Producto Bruto Interno y la tasa de interés de las empresas financieras peruanas del crédito de consumo durante el período de octubre 2002 – marzo 2014. Así mismo, evaluar a través del análisis de impulso – respuesta y la descomposición de varianza, el impacto de cada una de las variables sobre otra y su contribución a la desviación típica del error. Al realizar el análisis exploratorio de las variables involucradas, se encontró que tanto el porcentaje de morosidad como la tasa de interés muestran una tendencia creciente desde el 2011 en adelante. Finalmente, el modelo Vector Autorregresivo (VAR) que mejor se ajusta, es aquel que considera las primeras diferencias finitas de cada una de las variables bajo cuatro números de rezagos.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio Institucional - UNALM,Ingeniero Estadístico e Informático
"Porras Cerrón, Jaime Carlos","Tang Bedoya, Felix Augusto||Vargas Cuyan, Cecilia",2016,"El aumento continuo de grandes volúmenes de datos y la importancia de la utilización de éstos, junto con la búsqueda de satisfacción de los clientes se han vuelto un gran reto que las grandes empresas hoy en día quieren superar. En la actualidad las empresas conocen la importancia que tiene el almacenamiento, la captura de datos y el beneficio que le puede resultar si se explotan correctamente. Este estudio presenta una técnica estadística para segmentar clientes de una tienda de retail en el Perú. El conjunto de datos está conformado por clientes (personas naturales) que han adquirido un artículo para su hogar mediante un crédito en el primer trimestre del año 2013 de una tienda de retail. El conjunto de datos inicial estaba compuesto por 6284 clientes. Luego de proceder con el análisis exploratorio y la limpieza de datos, correspondiente a la eliminación de datos outliers y faltantes; se trabajó un conjunto de datos compuesto por 4980 clientes. Se aplicó análisis de conglomerados bietápico, que es una técnica de segmentación que permite trabajar con variables cuantitativas y categóricas. El resultado de la aplicación de la técnica brindó 3 conglomerados: el primero con 1817 clientes (36.5%), el segundo conglomerado con 1390 clientes (27.9%) y  el tercer conglomerado con 1773 clientes (35.6%). Después de esto se procedió a describir los perfiles de cada conglomerado y se propusieron estrategias de marketing mix para cada uno de ellos, con el objetivo de fidelizar al cliente, aumentar las ventas y el posicionamiento de la tienda de retail.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"López de Castilla Vásquez, Carlos","Quispe Quispe, Braulio",2016,"La presente tesis plantea una aplicación de los modelos estadísticos de procesos puntuales espaciales Poisson así como de los modelos Clúster del tipo Neyman - Scott. Particularmente, se enfoca en evaluar la distribución espacial de hechos delictivos y su relación con algunas covariables espaciales. De esta forma se permitirá orientar y/o establecer políticas referidas a seguridad ciudadana de índole nacional y/o local. El área de estudio corresponde a los distritos de Lima Centro y Residencial, para lo cual se toma en cuenta la información de ubicaciones georreferenciadas de los hechos delictivos reportados por las víctimas a finales del año 2013 hasta inicios del 2014. Las ubicaciones de los delitos son representadas por puntos, y el conjunto de estos se consideran un patrón puntual, el cual representa una realización de un proceso puntual espacial subyacente en el espacio de estudio. El modelamiento estadístico se realiza a través de la intensidad de puntos, la cual puede ser estimada para cualquier ubicación específica del área de estudio y son los modelos log-lineales los más usados para representar su relación con un conjunto de covariables espaciales cuyos efectos podemos representar en un conjunto de parámetros; a estos modelos se les conocen como modelos paramétricos de procesos puntuales espaciales. Las estadísticas de resumen, conocidas también como propiedades de primer y segundo orden de un proceso puntual así como los métodos basados en distancia entre puntos, han sido aplicados con fines de realizar el análisis exploratorio y determinar: el tipo de distribución espacial (regular, aleatorio o clústeres) que siguen los hechos delictivos (patrón puntual), la distribución de la distancia de un punto arbitrario a un lugar de ocurrencia de un delito y de la distancia de un hecho delictivo a otro, entre otras. Finalmente, se concluye que la distribución espacial de los hechos delictivos en Lima, no es homogénea, existiendo clustering o agregación de puntos, los cuales se traducen en zonas con mayor incidencia de hechos delictivos y su intensidad guarda relación con la ubicación de los límites distritales, la inversión destinada al orden interno y la densidad poblacional.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Maehara Oyata, Víctor Manuel","Rebaza Fernández, Diana del Rocío",2017,"La recurrencia de un evento en un paciente es la frecuencia observada de este en un periodo de tiempo durante el seguimiento al individuo, por ejemplo hospitalizaciones sucesivas de neumonía, episodios de epilepsia, recaídas de cáncer, entre otros. Los modelos de eventos recurrentes son muy útiles para la aplicación en estos fenómenos, y la presente investigación pretende ilustrar y comparar modelos particulares de datos de eventos recurrentes sin efecto aleatorio: Andersen y Gill (A-D); Wei, Lin y Weissfeld (WLW); y, Prentice, Williams  y Peterson (PWP), los cuales son modelos basados en la extensión de Cox de riesgos proporcionales, en estos modelos se asumen independencia de eventos. Otro modelo estudiado es el modelo de Fragilidad Compartida Gamma para eventos recurrentes que considera un término de fragilidad y asume que este término influye en la recurrencia de los eventos de un mismo sujeto. Para la estimación de los parámetros en los modelos sin efecto aleatorio se utilizó el método de máxima verosimilitud parcial mientras que para el modelo de fragilidad  fue el método de máxima verosimilitud penalizado, el cual penaliza la función de riesgo base. Los datos usados para la aplicación de estas metodologías fue proporcionada por el médico Ginecólogo Oncólogo Dr. Vladimir Villoslada Terrones del Instituto Nacional de Enfermedades Neoplásicas (INEN). Estos datos describen un conjunto de variables relacionados al cáncer de mama en una cohorte prospectiva de 68 pacientes con diagnóstico positivo, sometidos a una cirugía mastectomía. Al procesar y analizar los resultados obtenidos, se encontró que el modelo Andersen y Gill (A-D) y Prentice, Williams y Peterson (PWP) son los que ajustan mejor a este conjunto de datos. Entre los resultados encontrados se obtuvo que los factores asociados al riesgo de recurrencia de cáncer de mama son la edad de inicio al estudio, la edad de primera menstruación (menarquia) y tipo carcinoma lobulillar. Estos modelos presentan similares resultados debido a la significancia estadística en las variables y el cumplimiento del supuesto de riesgos proporcionales.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Menacho Chiok, César Higinio","Cano Alva Trinidad, Jesús María",2018,"Las instituciones de educación superior, cada vez más están implementando en sus diversos cursos evaluaciones virtuales vía web, con la finalidad de automatizar y medir con mayor precisión los conocimientos que van adquiriendo los estudiantes. La presente investigación, tiene como objetivo realizar un estudio comparativo de la Teoría Clásica del Test y la Teoría de Respuesta al Ítem, que pueden ser aplicados a los test informatizados con la finalidad de evaluar y medir las propiedades psicométricas y estadísticas, enfocando la confiabilidad y validez de los test de evaluación. Como caso de estudio, se usó un test informatizado de 30 preguntas (ítems) que se aplicó virtualmente a 775 estudiantes de una institución de educación superior matriculados en un curso de Estadística básica en el semestre 2016 II. El análisis de la confiabilidad, resultó con un alfa de Cronbach de 0.8325 pudiendo indicar una confiabilidad buena para la prueba informatizada, también fue corroborada con una correlación de Spearman-Brown de 0.815. El análisis con la TCT, el índice de dificultad identificó tres preguntas muy fáciles (V7, V8 y V12) que fueron retiradas, mientras el índice de discriminación no encontró ninguna pregunta con problemas. El supuesto de la unidimensionalidad de la prueba usando el análisis factorial fue probado con una variancia explicada del primer factor de 24.7%. El modelo logístico binario de la TRI que mejor se ajustó a los datos de la prueba fue el de tres parámetros (3PL). El proceso de calibración con el modelo 3PL, permitió retirar las preguntas V28 (índice de discriminación mayor 0.65) y V8, V12, V16 y V18 (índice del azar mayores a 0.4). Mientras que todas la preguntas estuvieron dentro del rango permitido para índice de dificultad",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Menacho Chiok, César Higinio","Aquino Gamboa, Juan Carlos",2019,"En la presente tesis se aplica la regresión logística ordinal, con la finalidad de identificar las variables que mejor explican los rangos de los ingresos de los egresados universitarios del Perú. Para el estudio se usa los datos de la encuesta de egresados universitarios del año 2014 realizada por el INEI. Se ajustan los datos a tres modelos aplicando la regresión logística ordinal con función de enlace logit proporcional acumulativo y para las universidades públicas y privadas. La variable dependiente son los rangos de ingresos (Bajo, Medio, Alto y Muy alto) y los conjuntos de variables independientes agrupadas en cuatro categorías: Grupo A: socio-académicas (7), Grupo B: referidas a la evaluación de las competencias recibidas en la universidad (12), Grupo C: referidas a la importancia de las competencias para su desarrollo profesional (12) y Grupo D: respecto a los profesores de la carrera (5). Para explicar los rangos de los ingresos, la regresión logística ordinal identificó para los tres modelos y para las universidades públicas y privadas, variables significativas socio-académicas: sexo, pertenencia al cuadro de méritos, si obtuvo o no el título profesional, su primer empleo relacionado con la formación profesional. Respecto a la calificación sobre la preparación recibida en la universidad para el desarrollo de las competencias: para coordinar actividades, para los conocimientos básicos de otros campos (públicas) y para el dominio del área de disciplina, el utilizar herramientas informáticas básicas, el utilizar software específico de la carrera (privadas). Respecto a la importancia de las competencias para su experiencia laboral: redactar informes o documentos, tener conocimientos básicos de otros campos o disciplinas (públicas) y rendir bajo presión y cumplir con los objetivos y coordinar actividades (privadas).",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
"Menacho Chiok, César Higinio","Reyna Colona, Tino Fabricio",2017,"En este tiempo presente, las empresas desarrollan diversos planes de trabajo conjunto de negocio, a modo de corporaciones, joint ventures, etc., y surge la necesidad de manejo eficiente y ordenado de volúmenes grandes de datos e intercambio de información entre las corporaciones con los fines de negocio visibles en el ámbito comercial. En tal sentido, el trabajo de investigación provee una metodología de desarrollo de un sistema de integración basado en Middleware Orientado a la Mensajería (MOM) que media y favorece la integración financiera entre dos entidades de ejemplo, una empresa farmacéutica y un banco. El propósito del trabajo de investigación es que, con la metodología presentada, se identifiquen los procesos clave y la dinamización de los mismos para representar el flujo de transacciones electrónicas para la debida integración entre las dos entidades señaladas anteriormente. La metodología consta de las secciones de: análisis del sistema, diseño del sistema, arquitectura y secciones referidas a la interconexión, mensajería e interfaces gráficas de usuario para la debida gestión del sistema. Los resultados de la aplicación de esta metodología son: el reconocimiento de los procesos clave según los objetivos del trabajo, la presentación de la dinamización de tales procesos, y comprender de manera preliminar lo referido a la interconexión entre las entidades presentadas, empresa farmacéutica y banco, definición de mensajes para las transacciones financieras entre las entidades señaladas y la visualización de la gestión del sistema por medio de interfaces gráficas de usuario. Tales resultados conducen a la idoneidad de la metodología en todas las secciones presentadas, abriendo el espacio para posteriores oportunidades de investigación en banca electrónica.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"Salinas Flores, Jesús Walter","Pariona Huarhuachi, Jefferson Clauss",2017,"La retención de clientes ha tomado mucha importancia en los últimos años en las entidades financieras debido a la competencia agresiva por parte del sector, así como la autonomía del cliente en buscar mejores beneficios dentro de todas las ofertas que existen en el mercado bancario lo que se ve reflejado en el aumento de la tasa de clientes fugados. Ante esto se ha visto necesaria la implementación de técnicas estadísticas y/o técnicas de minería de datos, con la finalidad de construir un clasificador predictivo que pueda ayudar a identificar a clientes potenciales a fugarse. En muchos casos cuando se aplican técnicas de clasificación, es común que la clase a predecir ocurra con menor frecuencia que la otra clase: la presencia de datos desbalanceados. Es decir, se tiene menor número de clientes fugados que no fugados, lo cual representa un inconveniente debido a que el clasificador necesita datos suficientes de ambas clases para poder aprender de ellas y así alcanzar una buena predicción. En esta investigación se propone el algoritmo Syntetic Minority Over-sampling Technique (SMOTE) como solución a este problema. SMOTE crea instancias nuevas a partir de un sobre-muestreo de las instancias existentes, llevando la clase minoritaria a un número suficiente para ser considerada balanceada y la clase mayoritaria si es necesaria reducirla mediante sub-muestreo aleatorio. En la presente investigación se validarán tales beneficios con la construcción de un modelo de regresión logística binaria con datos desbalanceados con y sin la aplicación del algoritmo de SMOTE; con el fin predecir la fuga de clientes en una entidad financiera. Se usarán para medir la precisión, la curva ROC y elementos de la comprobación de tabla cruzada como la especificidad y la sensibilidad.",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
"Chue Gallardo, Jorge","Muñoz Muñoz, Emanuel Guillermo",2015,"El propósito de este trabajo es presentar y aplicar la técnica de redes neuronales para predecir el éxito de la compra de deuda de una entidad financiera a otra. La técnica de redes neuronales se sustenta en el perceptron multicapa y en el algoritmo de retropropagación. Se explica el uso de la técnica mostrando sus diferentes pasos: establecimiento de la estructura, la función de activación (sigmoidea), el paradigma de aprendizaje, el factor de aprendizaje, la regla de aprendizaje, el algoritmo de aprendizaje (retropropagación), el entrenamiento y la evaluación de la red neuronal. Se realizó un pre procesamiento para tener datos de calidad, con estos datos limpios se aplicó la técnica de redes neuronales en la clasificación y para evaluar esta clasificación se realizó el post procesamiento, se puede notar que todo este proceso es minería de datos. Lo más resaltante en las redes neuronales son los valores hallados para los parámetros, identificados a través del entrenamiento. Estos parámetros son denominados pesos dinámicos, en estos pesos está el conocimiento de la red neuronal para poder lograr la predicción. El error de clasificación que se obtiene al aplicar a los datos de prueba a la red neuronal entrenada con las especificaciones ya señaladas fue de 22.89% y 4.31% para la red de cuatro y dos neuronas en la capa de salida respectivamente. Al probar con más de siete neuronas se obtuvieron errores de clasificaciones similares o mayores, esto se logró con un mayor costo computacional. Al comparar los resultados de los mismos datos con la regresión logística, se obtuvieron estos errores de clasificación 22.72% y 4.31% para los datos con la variable respuesta de cuatro y dos clases respectivamente.También evaluaron los modelos de clasificación con la técnica de validación cruzada, en este caso se obtuvo  76.57% y 77.29% que son los porcentajes de la eficiencia de predicción de redes neuronales y regresión logística respectivamente, solo para los casos donde la variable respuesta de los datos era de cuatro niveles",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Febres Huamán, Grimaldo","Orosco Gavilán, Juan Carlos",2019,"La presente investigación es de naturaleza aplicada, y tiene el objetivo de analizar y evaluar la metodología Bootstrap en modelos heterocedásticos aplicados en la predicción del Índice General de la Bolsa de Valores de Lima (IGBVL), periodo 2010 - 2014. Se presenta sucintamente, los conceptos básicos de series temporales, los procesos seriales heterocedásticos, la metodología Bootstrap y sus aplicaciones a la inferencia estadística y a las series temporales, donde se presenta el algoritmo para procesos heterocedásticos GARCH propuesto por Pascual et al. (2006) y generalizados para los modelos EGARCH y TGARCH. Con los procedimientos mostrados fueron obtenidas las predicciones mediante la metodología paramétrica y metodología Bootstrap, que fueron comparados con valores reales y finalmente fueron evaluados los desempeños de ambas metodologías. Del estudio se obtuvo que los modelos que mejor ajustan a la serie son los modelos ARMA(1,1)-GARCH(1,1), ARMA(1,1)-EGARCH(1,1) y ARMA(1,1)-TGARCH(1,1) cada uno de ellos con el supuesto de distribución t de Student con 5 grados de libertad de los residuales, el estudio comparativo mostró que la aplicación de la metodología Bootstrap en la serie de los retornos del Índice General de la Bolsa de Valores de Lima, permite obtener intervalos de predicciones con mayores e iguales amplitudes en algunos horizontes hacia adelante en comparación con la metodología paramétrica, y también permitió construir con un buen desempeño los intervalos de predicción para las volatilidades, así siendo esta una alternativa para la construcción de intervalos de predicción en los modelos GARCH, EGARCH y TGARCH.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Gonzáles Chavesta, Celso","Bazán Baca, Juan Francisco",2018,"El estudio ha tenido como propósito determinar el método de muestreo apropiado, así como realizar estimaciones de la desnutrición crónica (DC) por edades, sexo y gestión educativa. Se ha empleado el muestreo por conglomerados (escuelas) combinado con estratificado (gestión educativa) y selección sistemática; en una muestra de 16 escuelas de gestión estatal y 20 de no estatal, obteniéndose información de 3000 alumnos del primero al cuarto grado de primaria, de la Región Callao. Se analizó la relación talla/edad con la referencia del National Center for Health Statistic (NCHS) de los Estados Unidos de América y de la Organización Mundial de Salud (OMS) considerando como niño en DC aquel con talla debajo de menos dos desviaciones estándar. Se determinó que la talla promedio de los alumnos, tuvo un incremento de 3 cm. al 2016, respecto al año 2005. La DC el año 2016 alcanza el 2.5% (1455 alumnos) con el patrón del NCHS y 2.9% (1724 alumnos) con el de la OMS. Según la referencia NCHS, la DC baja en 4.1% el año 2016 (2.5%) respecto al año 2005 (6.6%). En este período, también baja la DC por edades, sexo y gestión de la institución educativa. El Odds Ratios de Prevalencia (ORP) con el patrón NCHS por sexo y gestión disminuye el año 2016 respecto al año 2005 a 1.27 y 2.29 respectivamente; resulta que un estudiante este desnutrido crónico es 1.27 veces más probable si es hombre a que sea mujer, y que un estudiante este desnutrido crónico es 2.29 veces más probable si estudia en una institución educativa estatal a que estudie en una no estatal. Para tener alrededor del 96% de casos válidos, para las edades de 6 a 9 años en la encuesta de talla a escolares del primero al cuarto grado de primaria de menores, es recomendable efectuarla en el primer semestre del año, preferentemente los meses de mayo y junio",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Porras Cerrón, Jaime Carlos","Asencios Gonzalez, Zaida Beatriz",2019,"En la presente investigacion, se empleo Modelos de Ecuaciones Estructurales con Minimos Cuadrados Parciales (PLS-SEM por sus siglas en ingles), es una tecnica de segunda generacion que utiliza metodos estadisticos para el analisis simultaneo de relaciones complejas entre dos a mas constructos latentes. Se aplico PLS-SEM con el objetivo principal de comprender como o por que medios la violencia contra las mujeres en relaciones de pareja (VcM, constructo o variable independiente) afecta a la productividad laboral (constructo o variable dependiente) medido en terminos de ausentismo y presentismo, y la explicacion de esta relacion es por medio del dano a la salud mental y fisica (constructo o variable mediadora). Para ello, se entrevistaron a 357 duenas de microempresas formales en 10 departamentos del Peru y a 977 duenas de microempresas informales o formales con acceso a credito en Paraguay, se aplico un cuestionario estructurado cuyas preguntas estuvieron medidas en escala ordinal. En el modelo de media, los resultados del PLS-SEM muestran que tanto en Peru y Paraguay los tres constructos analizados son validos y confiables, fundamentados por la fiabilidad compuesta, las cargas de los indicadores, la varianza extraida media (AVE), las cargas cruzadas, el criterios de Fornell Larcker y el Heterorrasgo-Monorrasgo (HTMT). En cuanto al modelo estructural, tanto en Peru como en Paraguay los hallazgos proporcionan evidencia empirica de que el dano a la salud mental y fisica explica la relacion entre VcM y productividad laboral. Por consiguiente, la presente investigación posee implicancias por la confirmacion del efecto mediador, la aplicacion de esta tecnica en este tipo de tematica y el desarrollo del marco teorico y practico del PLS-SEM.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Sotomayor Ruíz, Rino Nicanor","Castillo Gamarra, Jorge Enrique",2014,"El presente trabajo tiene como objetivo describir los modelos de varianza condicional ARCH y GARCH junto con sus propiedades y demostraciones, estos modelos se aplican en series de tiempo financieras, debido a que estas presentan como característica principal una fuerte volatilidad con periodos de calma o agitación, lo cual no permite utilizar los modelos de series de tiempo tradicionales que asumen varianzas constantes. Así mismo se realizó una aplicación utilizando como variable el valor diario del Índice General de la Bolsa de Valores de Lima (IGBVL), periodo 2009 – 2011, para la aplicación  se utilizó el software econométrico Eviews 7. Al analizar los resultados de las estimaciones de los modelos que explicarían la volatilidad diaria de la Rentabilidad del Índice General de la Bolsa de Valores (RIGBVL), periodo 2009 – 2011, se  concluyó que  el  modelo GARCH (1,1) es el adecuado, debido a que el modelo GARCH (1,1) tiene a diferencia de los demás modelos el menor valor tanto en el criterio de información de Akaike (AIC) como en el criterio de información de Schwarz. Previamente se modeló la media de la RIGVBL con el modelo AR (1)",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"Chue Gallardo, Jorge","Meza Rodríguez, Aldo Richard",2018,"La presente investigación tiene como propósito aplicar y comparar el modelo de regresión logística y el algoritmo Adaboost en datos desbalanceados, esto a efecto de predecir la fuga  de clientes en una empresa del sector de telefonía móvil. El algoritmo Adaboost se sustenta  en el aprendizaje adaptativo al entrenar clasificadores débiles combinándolos en conjunto  para obtener un clasificador cuyo rendimiento sea fuerte. En cuanto a la regresión logística su modelamiento se realizó estrictamente desde una perspectiva de minería de datos, donde la clasificación es el objetivo y el rendimiento se evaluó en un conjunto de validación. Ambas técnicas se compararon mediante dos procedimientos, el primero mediante métodos de muestreo (sub-muestreo, sobre-muestreo y SMOTE) y el segundo modificando y/o  ajustando el algoritmo o función. Al trabajar con datos desbalanceados la tasa de error de clasificación es ineficiente, por lo que las medidas de desempeño para elegir al mejor modelo fueron la precisión, el recall (sensibilidad), el F-measure, y como medida principal el AUC a través de curvas ROC. Al formar modelos logísticos con los métodos de muestreo, las medidas de desempeño arrojaron resultados similares, lo mismo pasó al formar modelos con el algoritmo Adaboost, sin embargo al comparar la regresión logística (AUC=0.86) con el algoritmo Adaboost (AUC =0.93), este último tuvo el mejor desempeño. En cuanto al ajuste a nivel de algoritmo o función, en la regresión logística se trabajó de dos maneras, el primero (Logit Asym) incluyendo en la FDA un valor Kappa (k) y el segundo (Power Logit) un valor Lambda (λ), en ambos modelos se identificaron los valores óptimos de k (0.02) y λ (2.5), en cuanto al algoritmo Adaboost (Adaboost Asym) se ajustó el peso de la clase minoritaria cuyo costo de clasificación fue errónea. La comparación de estos tres modelos ajustados dio como mayor rendimiento al algoritmo Adaboost. Finalmente se realizó la validación cruzada con 10 iteraciones para todos los modelos dando resultados similares al método de retención. Realizada todas las comparaciones y las medidas de desempeño se concluye que el modelo óptimo para la predicción de fuga de clientes en la empresa de telefonía es el algoritmo Adaboost",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Magister Scientiae - Estadística Aplicada
"Miranda Villagomez, Clodomiro Fernando","Chafloque Cespedes, María Raquel",2019,"Los Modelos de Ecuaciones Estructurales con Mínimos Cuadrados Parciales (PLS – SEM, por sus siglas en inglés), son un método de segunda generación, con gran aceptación en la actualidad en el mundo académico, en especial en el área de ciencias empresariales. Asimismo, el enfoque de esta técnica es más robusto y flexible al momento de utilizarlo en variables no observables. La presente investigación muestra una aplicación del PLS – SEM dentro del área de ciencias empresariales, en un sector económico donde no existe evidencia empírica cuando se habla de estrategias de marketing y desempeño empresarial. La investigación tuvo como objetivo determinar la relación entre la orientación de mercado, la innovación del producto y el desempeño de las empresas en el sector artesanal peruano - periodo 2018 mediante la aplicación del PLS-SEM. Se aplicó una encuesta estructurada a 301 microempresas del sector artesanal, específicamente las que están en el rubro comercial, siendo estas las principales intermediarias entre el consumidor final y el productor. Se encontró que la orientación de mercado y la innovación del producto explican el 34.3% del desempeño de la empresa; así mismo la orientación de mercado se relaciona positivamente a la innovación del producto, y esta última variable se relaciona positivamente al desempeño de la empresa. Se concluye que la técnica de PLS – SEM es adecuada para ser aplicada a investigaciones de ciencias empresariales. Finalmente, se recomienda que se incremente la evidencia empírica con el fin de fomentar el uso de esta técnica estadística de segunda generación.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Sotomayor Ruíz, Rino Nicanor","Loaiza Alamo, Marco Antonio",2015,"El objetivo de esta investigación es determinar la mejor selección de los siete programas especializados para la implementación de un laboratorio mediante la aplicación del Análisis de Proceso Jerárquico (AHP). Con esta técnica se logró un consenso para identificar cuáles son los criterios y las alternativas más relevantes para la toma de decisiones. Para validar el AHP se necesitó los Índices de Consistencia: el Cociente de Resistencia (CR) para la matriz de comparación de criterios y alternativas por pares, el Índice de Consistencia Geométrica (GCI) y el Indicador de Consenso AHP (S*) para la matriz consolidada. Con el desarrollo del algoritmo de AHP se optó por cinco criterios y cuatro alternativas para la toma de decisión. Se concluyó que la alternativa A, conformada por los programas informáticos Minitab, SPSS, SQL, Eviews y Microsoft Project, fue la más importante. No obstante, no hubo diferencia significativa considerable con la alternativa C, conformada por los programas informáticos Minitab, SPSS, SQL, Eviews y QlikView. En base a los resultados obtenidos se concluye que los programas más adecuados para la implementación del laboratorio informático son: Minitab, SPSS, SQL, Eviews, Microsoft Proyect y QlikView",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
,"Mamani Tone, Edith Rita",2017,"La presente monografía estudia la Descripción Metodológica del Modelo de Ecuaciones Estructurales con el Método de Estimación de Mínimos Cuadrados Parciales. En el primer capitulo se describe detalladamente la sustentación teórica del Modelo de Ecuaciones Estructurales; en el capítulo 2 se describe el Método de Mínimos Cuadrados Parciales (PLS); en capítulo 3 se describe la Metodología detallando los pasos del Modelo de Ecuaciones Estructurales por el Método de Mínimos Cuadrados Parciales y en el capítulo 4 se observa la Aplicación, paso a paso en un caso para ejemplificar la metodología. Se concluyó los modelos de ecuaciones estructurales SEM es una extensión de la regresión múltiple, se aplica esta técnica para encontrar relaciones entre variables observables y no observables llamadas (latentes) para pasar posteriormente a estimar los parámetros. tiene como objetivo la predicción, no es preciso que los datos provengan de una distribución normal y puede aplicarse a estudios de muestras pequeñas, permite estimar modelos muy complejos con muchas variables latentes y medibles. En la aplicación de la descripción metodológica se realizó un estudio empírico el segundo semestre de 2013 sobre una muestra correspondiente a 300 alumnos universitarios chilenos con acceso a bases de datos científicas. Para los cálculos de PLS se utilizó el software WarpPLS 4.0. Los resultados del ejemplo en el análisis de PLS del caso indicaron la buena capacidad predictiva del modelo de investigación, y a su vez, la explicación del análisis logró ejemplificar en forma clara la metodología propuesta.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"Salinas Flores, Jesús Walter","Elguera Vega, Rhony Miguel",2018,"En la actualidad, la gran cantidad de datos que se almacenan de los clientes en las diferentes empresas y la capacidad de procesamiento que brindan las computadoras, han generado gran interés por investigar; así como, desarrollar métodos y algoritmos para el análisis de agrupamiento. Los métodos de agrupamiento dirigidos a la segmentación de clientes permiten a las empresas identificar los patrones y perfiles de compra o servicios, ayudando a tomar mejores decisiones de las estrategias de canales y publicidad para sus clientes. En la presente investigación se aplica el método de agrupamiento basado en las particiones de k-Medoides con el algoritmo PAM (Partición Alrededor de Medoides). El algoritmo PAM se basa en particionar el conjunto de datos en k grupos, donde k es conocido; es considerado más robusto ante datos atípicos y el ruido, se basa en minimizar la suma de disimilitudes entre un objeto y el Medoide (centro del grupo). El objetivo de la presente investigación es aplicar el algoritmo PAM para segmentar a los clientes de un casino con los datos obtenidos, a través del uso de tarjetas en el tragamonedas. El método de la silueta permitió identificar tres clústers como el número óptimo. El análisis de agrupamiento con el algoritmo PAM usando la medida de distancia Gower, resultó la segmentación de clientes para los tres clúster con porcentajes de 49.4%, 11.3% y 39.4% respectivamente. La agrupación fue validada, al obtener para las 6 variables cuantitativas todos los ANVAs significativos y con el árbol de clasificación C5.0 un 99.35% de precisión.  Los resultados de la caracterización muestran que el clúster 1 son clientes con valores de los promedios para las 6 variables en un nivel intermedio, el 67.0% son hombres y 100% el tipo de tarjeta es classic. En el clúster 2 están los clientes con los valores más altos en los promedio de las 6 variables, el 59% son hombres y el 100% usan la tarjeta silver. En el clúster 3, se encuentran los clientes con los promedios más bajos, el 64% son hombres y el 100% usan tarjeta classic",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
"Miranda Villagomez, Clodomiro Fernando||Acevedo Mallque, Moisés Pascual","Montenegro Muro, Rolando Antonio",2018,"El bosque amazónico cuenta con una gran variedad de especies arbóreas, la cual se estima en cuatro mil especies. Unas pocas especies amazónicas enfrentan la desaparición por la alta demanda de su madera. Para reducir la presión sobre las especies demandadas y promover el uso de nuevas especies es necesario conocer sus características tecnológicas, principalmente, las características físico mecánicas. Debido a la gran cantidad de propiedades que encierran dichas características, se propuso estudiar a las especies con técnicas multivariadas, específicamente a través del análisis de conglomerados. Ello con la finalidad de agruparlas en función a la similitud que tengan en sus propiedades físicas y mecánicas. Así, se pueden agrupar especies poco conocidas en el mercado con especies muy demandadas y sugerir potenciales usos. Para el estudio presente se utilizó el algoritmo CLARA (Clustering Large Applications), el cual es empleado en grandes conjuntos de datos. Para seleccionar el número de conglomerados óptimo se probó hacer de dos hasta diez grupos; luego se comparó el ancho de la silueta promedio y el índice de Dunn por grupo y se eligió el de valores más altos. Se encontró que con un ancho de la Silueta promedio de 0,339 el número óptimo de conglomerados es de dos. El número de conglomerados indicado coincide con el análisis realizado a partir del índice de Dunn, el cual alcanza su más alto valor en 0,1264 con dos clústeres. Los conglomerados tuvieron como medóides a Guarea subridiflora (“requia de altura”) y Retrophyllum tospigliosii (“ulcumano). El primer conglomerado se caracterizó por tener propiedades mecánicas y físicas altas, de acuerdo a lo establecido por Aróstegui et al (1986). Por otro lado, el conglomerado de medóide “ulcumano” se caracterizó por tener propiedades físico mecánicas bajas, a excepción del clivaje, el cual resultó ser medio.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Chue Gallardo, Jorge","Valdivia Carbajal, Manuel",2019,"Esta tesis toma como caso de estudio a una empresa de cosméticos reconocida de la ciudad de Lima, Perú. Para pronosticar el riesgo de crédito se analizaron dos modelos: la Regresión Binaria Asimétrica Cloglog y las Redes Neuronales Artificiales Perceptrón Multicapa. La selección de estos modelos surge a raíz de recientes estudios que revelan las ventajas de las técnicas de inteligencia artificial sobre los modelos estadísticos en cuanto a predicción por su alta capacidad de discernimiento de patrones. “La empresa” cuenta con un modelo de negocio llamado Red Binaria, esto quiere decir que se contrata vendedoras y éstas ofrecen productos a sus clientes a través de catálogos. Debido a que no se cuenta con información de los clientes finales, se midió la probabilidad de no pago a través de las vendedoras. La población de estudio estuvo conformada por las vendedoras de la empresa las cuales manejan una cartera de clientes de 51183 personas a julio del 2017. Los datos se trataron previamente considerando el análisis de valores atípicos a nivel univariado y multivariado, este último mediante el algoritmo de segmentación K-means. Concluido ello para realizar la clasificación de vendedoras en buenas y malas pagadoras se utilizó un modelo de Redes Neuronales Artificiales Perceptrón Multicapa con una sola capa intermedia y un modelo de regresión Binaria sobre el cual se eligió el enlace asimétrico Cloglog debido a la naturaleza de los datos. Los resultados mostraron un 0.846 y 0.809 de índice ROC en las muestras de entrenamiento, y un 0.762 y 0.733 de índice ROC en las muestras de testeo respectivamente para cada modelo. Finalmente, se concluye que la aplicación de la técnica de Redes Neuronales Perceptrón Multicapa define una mejor regla de discriminación que la Regresión Binaria Asimétrica Cloglog en el estudio de probabilidad de impago. Además, las Redes Neuronales presentan mejores indicadores de pronóstico.",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Magister Scientiae - Estadística Aplicada
"Miranda Villagomez, Clodomiro Fernando","Málaga Sabogal, Lucía",2017,"Este estudio halló los grupos de investigación conformados por las instituciones peruanas con investigación en medicina indizada en Scopus en base a las coautorías. La información se descargó de Scopus en formato no estandarizado y se utilizó aprendizaje supervisado con k-medias y un conjunto de datos de entrenamiento, para la identificación de las instituciones involucradas. El procesamiento de los datos se hizo con R. Las instituciones identificadas se clasificaron en ocho categorías: universidades, institutos públicos de investigación, clínicas y hospitales, organismos y dependencias del gobierno nacional, organismos y dependencias del gobierno local, empresas, organizaciones internacionales con filiales en Perú, instituciones privadas sin fines de lucro; y dos sectores: público y privado. Posteriormente se identificó los conglomerados existentes utilizando la metodología de particionamiento jerárquico aglomerativo propuesta por Moore, Clauset y Newman e implementada en el paquete igraph en R. Se halló que las instituciones del sector salud tienden a colaborar con sus símiles pero que no existe relación entre el tipo y sector de la institución y los patrones de colaboración para otras instituciones",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Maehara Oyata, Víctor Manuel","Vargas Paredes, Ana Cecilia",2017,"Se estimó mediante un modelo lineal mixto los componentes de varianza y heredabilidad de la producción de leche, a partir de los registros de 3397 lactaciones, provenientes de 1359 vacas de raza Holsteins, de 57 rebaños con información genealógica de 5 generaciones, utilizando máxima verosimilitud restringida conocida como REML y muestreo de Gibbs basado en procedimientos bayesianos. Con ambas metodologías se obtuvo una heredabilidad, en sentido amplio, moderada de 0.135 vía REML y una media de 0.318 vía muestreo de Gibbs. Para realizar el análisis exploratorio de residuales (en función de los tres tipos: marginal, residual condicional y efectos aleatorios) del modelo lineal mixto estimado vía REML, se adaptó funciones en R para incorporar la información genealógica o pedigrí al modelo. Como resultado de esto se verificó la linealidad de los efectos fijos y la normalidad del componente genético del animal. No se encontró normalidad para el efecto aleatorio del rebaño ni para los residuales condicionales. Para  estos últimos tampoco se observó homocedasticidad. Además, se encontró que para 132 animales la estructura de covarianza considerada en el modelo no es adecuada. También, se observó 215 animales y 7 rebaños con efectos atípicos. En el diagnóstico del procedimiento de simulación del muestreo de Gibbs desde la perspectiva bayesiana no se encontró problemas de convergencia. Se obtuvieron errores de Montecarlo bajos y tamaños efectivos de muestra mayores a 1000 para cada componente del modelo.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Magister Scientiae - Estadística Aplicada
"Gonzáles Chavesta, Celso","Recuay Denegri, Briggitt Giuliana",2017,"Ante un mercado cambiante las empresas del rubro de consumo masivo se encuentran sumergidas en una serie de interrogantes en cuanto a la participación de sus productos. Para ello aplican herramientas como la auditoría de mercado que a través del resultado de una de sus etapas el “análisis de datos” encuentran confianza a la hora de tomar decisiones. Este estudio presenta el proceso del análisis de datos en una auditoría de mercado para productos de consumo masivo en el canal tradicional. Los productos analizados fueron las cremas dentales, los desodorantes y los energizantes. Se trabajó con una muestra de 1200 bodegas ubicadas dentro de Lima Metropolitana. El conjunto de datos contiene variables cuantitativas como las ventas, las compras, los precios y los inventarios, además de las variables cualitativas como son los atributos (marca, tamaño, sabor, etc.) de las variedades. Los datos fueron recolectados de las bodegas el primer semestre del año 2016 vía celular mediante el aplicativo de relevamiento “Audit” para luego ser cargados al sistema de trabajo. Culminada cada fase del análisis de datos (limpieza de datos, análisis de las variables y visualización de resultados) se supervisaron las bodegas cuyas variedades presentaron incoherencias en sus datos y se procedió a corregir. Para los tres productos en su mayoría se encontraron: ausencia de datos en la variable compra e inventario y errores en los precios y ventas. Estos errores suelen suceder debido a una mala digitación (error de codificación) o a la omisión de parte del supervisor al momento de ingresar los datos al aplicativo. Como resultado del estudio se obtuvieron datos consistentes para las cremas dentales, desodorantes y energizantes, los cuales fueron de mucha relevancia para las empresas. Otros productos de consumo masivo pueden seguir también la misma estructura para el análisis de sus datos.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"Sotomayor Ruíz, Rino Nicanor","Chumpitaz Ramos, Domingo Guzmán",2021,"Los Modelos de Ecuaciones Estructurales (SEM), es una extensión de varias técnicas multivariantes, entre ellas el Análisis Factorial, se ha utilizado casi en todos los campos de estudio, principalmente en el área de la educación, esta técnica nos proporciona un método directo para tratar múltiples relaciones de variables observables y no observables. El objetivo principal de la investigación fue determinar la relación entre la calidad de servicios con los factores que la determinan (tales como la seguridad, fiabilidad, empatía, aspectos tangibles y capacidad de respuesta); y la satisfacción estudiantil universitaria utilizando Modelos de Ecuaciones Estructurales (SEM). En la investigación se diseñó un cuestionario en base al instrumento de calidad de servicio SERVQUAL relacionado a un modelo estructural teórico del autor Lobos y Sepúlveda. Se hizo un análisis de fiabilidad del cuestionario, y con el análisis factorial exploratorio se demostró su validez. Luego, se recolectó información de 158 estudiantes del Área de Ciencias Económicas y de la Gestión de la UNMSM. Para desarrollar la investigación se aplicó el Análisis Factorial Confirmatorio (AFC) y Modelos de Ecuaciones Estructurales (SEM). En el modelo estructural inicial, los constructos empatía y fiabilidad no fueron significativos, entonces se reespecificó el modelo con los índices de modificación. En el modelo estructural 2, las variables observadas X5, X6 y X12 no fueron significativas en los constructos de fiabilidad y empatía, por lo tanto se retiraron del modelo. En el modelo estructural reespecificado 3, empatía no fue significativo, entonces no fue considerado en el siguiente modelo. En el modelo estructural final los constructos de seguridad, fiabilidad y aspectos tangibles fueron significativos con un nivel de significancia de 0.05. La calidad de servicio esperada tiene una relación directa con la satisfacción estudiantil y, los constructos de seguridad, fiabilidad y aspectos tangibles están relacionados positivamente con la calidad de servicio esperada.",Universidad Nacional Agraria La Molina,,Magister Scientiae - Estadística Aplicada
"Menacho Chiok, César Higinio","Alarcón Pimentel, Sandra Elena",2021,"El sector asegurador peruano es supervisado en todo momento por la Superintendencia de Banca y Seguros – SBS. Esta entidad vigila y evalúa el comportamiento, prácticas e información relacionada a las aseguradoras con el objetivo de fomentar la rentabilidad, transparencia y una mayor protección de asegurados y beneficiarios. Para lograrlo las aseguradoras deben manejar métodos de estimación de provisiones (reservas) técnicas que contengan fundamentos estadísticos y que los resultados obtenidos sean lo más certero posible. En esta monografía, se presenta la metodología y la aplicación del método estocástico Double Chain Ladder - DCL para el Seguro Obligatorio de Accidentes de Tránsito – SOAT, siendo este tipo de método el más ventajoso para la estimación de pagos futuros de siniestros. Los resultados con la metodología DCL para estimación de la provisión de pagos futuros de los accidentes en el SOAT para la empresa aseguradora, se obtuvieron valores estimados para las reservas de siniestros ocurridos (RBNS) de S/. 293,206 y para las reservas de siniestros ocurridos y no reportados (IBNYR) de S/. 48,826 y para el total de las  reservas IBNR en S/. 342,033. En comparación, con el método Double Chain Ladder se obtuvo una reserva de S/. 342,033 menor que con el método Chain Ladder S/349,117. En la repartición de los siniestros, el 90% es de las reservas son RBNS y un 10% de las IBNYR, lo que implica que la gran mayoría de siniestros ya han sido notificados a la compañía, pero hay un retraso en la liquidación",Universidad Nacional Agraria La Molina,,Ingeniero Estadístico e Informático
"Gamboa Unsihhuay, Jesús Eduardo","Romero Cuadros, Italo Brayan",2022,"Esta investigación tiene como objetivo de determinar los patrones de clasificación del desempeño del sector público mediante la técnica de análisis conglomerados de series de tiempo a las regiones del Perú para el periodo 2007 – 2019. El desarrollo se realizó con los indicadores creados bajo la metodología de la frontera de posibilidades de producción y se evaluó la asociación a través del tiempo con el método de distancia de deformación del tiempo. El indicador utilizado para decidir el número de los conglomerados fue el de Silueta y Calinski. Entre los resultados más importantes se encontró que se mantiene un patrón que predomina entre las regiones que mejor usan los recursos y obtienen resultados idóneos donde resalta Moquegua, Ica y Lima mientras que otro grupo que mantiene fuertes tendencias a ser ineficientes en comparación como Ayacucho y Huancavelica. Finalmente, se observó una predominancia en las regiones que subdivide entre dos conglomerados y se mantiene tanto sectorial y global.",Universidad Nacional Agraria La Molina,,Magister Scientiae - Estadística Aplicada
"Salinas Flores, Jesús Walter","Marcos Sánchez, Eduardo Angelo",2015,"El crecimiento de los créditos por consumo a nivel mundial, junto con la normativa internacional sobre requerimientos de capital (Basilea II), están impulsando a las instituciones de banca retail a una mayor competencia con las entidades bancarias por este segmento de negocio. Por lo tanto en la empresa Carsa S.A.C. tiende a utilizar estrategias de ventas a crédito para tener un mayor crecimiento en este mercado competitivo, sin embargo su falta de control e identificación de clientes buenos y malos hace que sus estrategias puedan causar grandes pérdidas. Por ello, el objetivo de investigación del estudio es proponer un modelo de regresión logística, con el fin de analizar el riesgo crediticio en las ventas al minoreo en la empresa Carsa S. A. C. La investigación es de tipo descriptiva, explicativa transversal ya que comprende una población de los créditos generados en el periodo de enero y noviembre del año 2013 de electrodomésticos vendidos a nivel nacional. El modelo de regresión logística encontrado está compuesto por las variables: tipo de actividad, línea del producto, tipo de propiedad, estado civil, total de créditos, productos crediticios, plazo del crédito y mora máximo en el sistema financiero. En consecuencia, la empresa Carsa S. A. C. debe incluir el modelo de credit scoring en su política de crédito como un filtro adicional al otorgamiento convencional del mismo, dicho modelo es el cual discrimina a los clientes buenos y malos basándose en el perfil del cliente previo al otorgamiento de crédito. En la actualidad, la empresa analizada tiene grandes pérdidas por clientes morosos o mal identificados, efecto causado por el otorgamiento del crédito por el método por experiencia. Las pérdidas monetarias de solicitudes que debieron ser rechazadas en el 2013 ascienden a S/. 658,000.00 nuevos soles.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"Sotomayor Ruíz, Rino Nicanor","Prado Pariona, Vanesa||Palomino Quispe, Jacqueline Roxana",2015,"El trabajo, consistió en detallar paso a paso la metodología (CRISP–DM) para poder identificar grupos óptimos de clientes más propensos a migrar de un plan prepago a postpago con el fin de formular un plan de mejora en la gestión de llamadas mediante la clasificación de la base de datos. Este trabajo ha sido motivado por que actualmente se ha visto una disminución de la tasa de efectividad y contactabilidad con los clientes, para esto se ha utilizado el software Rapid Miner ya que es más detallada la representación de flujos de manera gráfica y por su gran capacidad para trabajar con una amplia gama de bases de datos. Se aplicaron modelos de clasificación para analizar las características que genera la compra de los diferentes servicios. Se realizó la comparación del modelo de Regresión Logística y el algoritmo de Árbol de Clasificación CART, quedando como modelo más óptimo la Regresión Logística ya que ofreció mejores resultados y mayor efectividad. A partir de lo anterior, se encontraron grupos diferenciados por las probabilidades de éxito venta (Migrar de un plan prepago a postpago), segmentos que reflejan necesidades y características particulares, que permita diseñar acciones de marketing focalizado con el objetivo de incrementar la tasa de efectividad, contactabilidad e incrementar las ventas. Se realizaron recomendaciones para futuras acciones de marketing, un ejemplo es identificar grupos que se debe intentar desarrollar y otros grupos que sólo que se debe tratar de fidelizar, ya que han alcanzado gran parte de su potencial dentro de la empresa. Cómo trabajos futuros se recomienda replicar la metodología con mayor información demográfica, con el fin de aumentar los índices de desempeño de los modelos predictivos. Además de poder cuantificar el aumento de la efectividad debido a la aplicación de esta metodología, a través de una campaña real.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
"Sotomayor Ruíz, Rino Nicanor","Bernabé Ponte, Eduardo||Ballón Beltrán, Dante Daniel",2015,"En el presente estudio se tiene como objetivo primordial describir y predecir la vía de culminación de parto en gestantes (Normal o por cesárea) mediante un modelo logístico binario y como objetivo secundario identificar los factores asociados que permiten determinar si una gestante tendrá una vía de culminación de parto: normal o por cesárea. Luego se realizará el análisis de regresión logística binaria con el fin de encontrar el mejor modelo que explique la variable de interés en función a las siguientes variables independientes: complicaciones de la madre, edad de la madre, edad gestacional, número de bebes, peso del bebe por nacer, talla del bebe por nacer y perímetro cefálico  del bebe por nacer. Los principales resultados que presentamos son: Tasa de clasificación según vía de culminación del parto en gestantes 88.5%; el coeficiente de determinación del modelo 65.9%, pruebas estadísticas de Hosmer y Lemeshow 0.760, lo cual verifica que nuestros datos se ajustan al modelo logístico binario, la tasa de mala clasificación del modelo validada mediante validación cruzada con 11.65%, siendo un error de estimación bajo y aceptable.",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
"Salinas Flores, Jesús Walter","Vizcarra Mac, César Augusto||Lajo Salazar, Julian Javier",2016,"En el siguiente trabajo se busca resolver un problema que ocurre a menudo en diversas empresas, especialmente dentro del área de marketing, mediante el uso de tres técnicas estadísticas. Aplicándolas adecuadamente se encontraron nichos de usuarios del servicio de telefonía móvil, a los cuales se les identificaron perfiles que los describieran adecuadamente y finalmente se buscaron que estas agrupaciones contribuyan a obtener un nivel de eficiencia mayor al realizar alguna acción de marketing específica. Durante el análisis se encontraron tres grupos de personas, estos grupos ayudarán a realizar un marketing mejor enfocado a cada tipo de cliente y así obtener mejores resultados, ya sea en venta como en inversión publicitaria. La optimización de estos recursos servirá a las empresas de servicio de telefonía móvil para poder llegar a los usuarios de una manera más directa en lugar de realizar acciones de marketing masivo los cuales muchas veces son poco rentables. El primer grupo que se encontró es el de los TRADICIONALES. Son aquellas personas que les importan poco tener lo último en tecnología pero buscan que su teléfono móvil funcione como debe ser y les permita algo más que llamar y enviar mensajes. Valoran que los demás los reconozcan por usarlo pero prestan poca atención sobre la marca que usan. El segundo grupo que se encontró es el de los MODERNOS. Ellos valoran que su equipo luzca moderno y sea de una marca reconocida. Quieren algo que les dure mucho tiempo, pero conocen muy poco o nada sobre todas las funcionalidades. En muchos casos no saben usarlas. Además, les importa mucho ser reconocidos por las características físicas exteriores de sus teléfonos móviles. Por último se tiene al grupo de los TECNOLÓGICOS. Ellos no solo buscan un teléfono móvil tecnológico acorde a sus necesidades sino que también saben aprovechar sus funcionalidades (conocen bastante sobre tecnología). Ser reconocidos por lo que usan es algo adicional pero no primordial, valoran mucho más la calidad y seguridad que una marca reconocida les puede brindar. Una vez descritos estos grupos de usuarios, las empresas que empiecen a realizar acciones de marketing específicas para cada grupo lograrán estar por delante de la competencia ya que al tener una comprensión más profunda de cómo es que piensan los usuarios de telefonía móvil podrán generar estrategias más directas acorde a las necesidades del consumidor.",Universidad Nacional Agraria La Molina,Repositorio institucional - UNALM||Universidad Nacional Agraria La Molina,Ingeniero Estadístico e Informático
"Gonzáles Chavesta, Celso","Yopán Comeca, Herbert Alfonso||Gálvez Castillo, Renato Martín",2014,"El trabajo nace con la necesidad de mejorar el proceso de la confección de las prendas de vestir el cual origina un alto porcentaje de prendas de segunda calidad generando así grandes pérdidas económicas (US$ 100,000 aprox. al año) y dañando la imagen de la empresa con sus clientes los cuales en su mayoría son de Europa, EEUU y Canadá. Hay que tener siempre en cuenta que los clientes establecen un máximo permitido de prendas defectuosas por pedido, un porcentaje mayor conlleva a penalidades y una baja puntuación por parte de los clientes al momento de las encuestas de satisfacción.  Por tal motivo la empresa implementó la metodología Seis Sigma para ofrecer un mejor producto o servicio al cliente, de una manera más rápida y al costo más bajo posible, la cual se inició el 04 de Marzo del 2013 y finalizó el 26 de Julio del 2013. Se empezó con una breve descripción del proceso de confección de prendas de vestir y las operaciones en el que está involucrado; luego se ejecutó el ciclo DMAIC: Definir, Medir, Analizar, Implementar (Mejorar) y Controlar. En la fase de Definición; se identificó las necesidades del cliente y los requerimientos críticos del producto (medidas de la prenda, costuras, modelo solicitado y doblado para despacho), como también de los principales defectos que generan prendas de segunda calidad, por lo que se utilizó diagramas SIPOC y gráficos de Pareto. En la fase de Medición; se identificó y cuantificó las variables más relevantes del proceso de confección de prendas a controlar. Mediante el uso de herramientas de calidad como el estudio Gage R&R, gráficos de control, análisis de la capacidad del proceso y el nivel Seis Sigma se obtuvo la situación actual por la que atraviesa el proceso en estudio. El porcentaje promedio de productos no conformes por  prendas de segunda calidad es del 0.90% con un nivel Six Sigma de 0.89 y un CPk de -0.20 lo cual nos indica que el proceso no es capaz de cumplir con los requerimientos del cliente, esta incapacidad del proceso se refleja en el alto porcentaje de piezas defectuosas por millón (PPM) que es de 72.92%.En la fase de Análisis; se identificó las causas raíces que originan el problema, para ello se aplicó análisis de correlación, análisis de regresión múltiple y prueba de hipótesis que ayudaron a identificar a la polivalencia de los costureros, el estado de las máquinas de coser, el número de la muestra de costura y la calidad de agujas como los factores mas importantes que afectan al porcentaje de prendas de segunda calidad. En la fase de Mejora, se ejecutó las acciones en base a los resultados obtenidos en la fase de análisis con el objetivo de optimizar el proceso de confección de prendas de vestir; se plantean planes de acción; donde se ejecutan programas de capacitación al personal y se elaboran instructivos de trabajo para el proceso de costura. La fase de Control; es la última de las fases y busca mantener los resultados obtenidos en la fase de mejora, para tal efecto calculamos el nuevo promedio y nivel Seis Sigma, obteniéndose los valores de 0.59% de prendas de segunda calidad y 3.89 nivel sigma respectivamente reduciendo significativamente el porcentaje de piezas defectuosas por millón (PPM) a 0.84%. Para tener bajo control el indicador y asegurar que mejore el tiempo, se diseñó planes de capacitaciones continuas a los 3 niveles operativos de la planta (operarios, supervisores, jefaturas). Es así que se asegura que el porcentaje de prendas de segunda calidad por defectos de costura se mantenga dentro del intervalo planteado.   Finalmente; en la evaluación económica realizada podemos notar claramente la reducción significativa de los costos de calidad, reduciéndose en promedio de S/. 24, 830  a S/. 10,100 al mes; esto significa una reducción del 60% de los costos de calidad. Si súmanos a esto la mejora de la imagen de la empresa entre sus clientes reflejadas en la disminución de penalidades por productos no conformes y una mayor puntuación en las encuestas al final de cada semestre el beneficio es aún mayor. En consecuencia, la ejecución del proyecto utilizando la metodología Seis Sigma ayudó a obtener beneficios económicos esperados.",Universidad Nacional Agraria La Molina,Universidad Nacional Agraria La Molina||Repositorio institucional - UNALM,Ingeniero Estadístico e Informático
